{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b00335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort Different columns: \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit_aer.primitives import Sampler\n",
    "import re\n",
    "import openpyxl\n",
    "\n",
    "df = pd.read_csv('Files/student_dataset.csv')\n",
    "type = detectColumns(df,df.columns)\n",
    "print(type)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a003e305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Student_Names   Phone_No.  Math  Physics  Chemistry Grade  \\\n",
      "0        Brenda Smith  9839882274    75       78         94     A   \n",
      "1       John Bradford  9509725199    89       96         85     A   \n",
      "2      Morgan Brennan  9953603562   100       57         86     A   \n",
      "3     Chad Washington  9065309878    77       95         96     A   \n",
      "4        Tracey Davis  9373998925    98       93         63     A   \n",
      "...               ...         ...   ...      ...        ...   ...   \n",
      "8995    Olivia Graham  9686810896    29       26         38     F   \n",
      "8996    Paul Chandler  9025870117    15       22         53     F   \n",
      "8997   Pamela Mcclain  9191166581    11       60         17     F   \n",
      "8998  Michael Farrell  9058455314    29       25         31     F   \n",
      "8999    Robert Rivera  9512623525    16       24         11     F   \n",
      "\n",
      "                   Comment  Roll No.           School Name  \\\n",
      "0     Very Good Achivement    581283  Martin Luther School   \n",
      "1     Very Good Achivement    584581  Martin Luther School   \n",
      "2     Very Good Achivement    515123  Martin Luther School   \n",
      "3     Very Good Achivement    529160  Martin Luther School   \n",
      "4     Very Good Achivement    558834  Martin Luther School   \n",
      "...                    ...       ...                   ...   \n",
      "8995                Failed    507991  Martin Luther School   \n",
      "8996                Failed    507160  Martin Luther School   \n",
      "8997                Failed    562264  Martin Luther School   \n",
      "8998                Failed    516548  Martin Luther School   \n",
      "8999                Failed    509986  Martin Luther School   \n",
      "\n",
      "                                        Student Address  \n",
      "0                      Unit 4373 Box 5591, DPO AP 75808  \n",
      "1     85792 Barnes Alley Apt. 473, Jenniferfurt, NC ...  \n",
      "2     7720 Scott Garden Apt. 717, Port Allisonburgh,...  \n",
      "3          968 Nancy Underpass, Turnerchester, FM 73242  \n",
      "4         43642 Payne Forest, East Cindymouth, NV 28190  \n",
      "...                                                 ...  \n",
      "8995          8939 Meyer Harbors, Timothystad, NE 62337  \n",
      "8996  953 Scott Centers Suite 398, Shawborough, MA 8...  \n",
      "8997   916 Max Orchard Suite 336, East Joseph, MD 78200  \n",
      "8998              15398 Anna Forges, Joneston, TX 66292  \n",
      "8999          1710 Austin Key, North Jonathan, NM 30342  \n",
      "\n",
      "[9000 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def digitSorting(df,col,asc):\n",
    "    df = df.sort_values(by=col,ascending=asc, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv(\"Files/student_dataset.csv\")\n",
    "df = digitSorting(df,'Grade',asc=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1b22e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[239]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m     38\u001b[39m df = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mFiles/student_dataset.csv\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Your dataset with 'Roll No.'\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m df_sorted = \u001b[43msortRollCol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGrade\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_sorted)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[239]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36msortRollCol\u001b[39m\u001b[34m(df, col)\u001b[39m\n\u001b[32m      8\u001b[39m digits_df = df[col].astype(\u001b[38;5;28mstr\u001b[39m).apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: pd.Series(\u001b[38;5;28mlist\u001b[39m(x)))\n\u001b[32m     10\u001b[39m digits_df[\u001b[33m'\u001b[39m\u001b[33moriginal_index\u001b[39m\u001b[33m'\u001b[39m] = df.index\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m sorted_digits_df = \u001b[43mrecursiveSort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdigits_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Use the sorted indices to reorder the original dataframe\u001b[39;00m\n\u001b[32m     15\u001b[39m sorted_indices = sorted_digits_df[\u001b[33m'\u001b[39m\u001b[33moriginal_index\u001b[39m\u001b[33m'\u001b[39m].values\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[239]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mrecursiveSort\u001b[39m\u001b[34m(df_digits, col)\u001b[39m\n\u001b[32m     29\u001b[39m result = []\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m value, group \u001b[38;5;129;01min\u001b[39;00m df_digits.groupby(col, sort=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     sorted_group = \u001b[43mrecursiveSort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     result.append(sorted_group)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.concat(result, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[239]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mrecursiveSort\u001b[39m\u001b[34m(df_digits, col)\u001b[39m\n\u001b[32m     31\u001b[39m     sorted_group = recursiveSort(group.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m), col + \u001b[32m1\u001b[39m)\n\u001b[32m     32\u001b[39m     result.append(sorted_group)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m op = \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[39m, in \u001b[36m_Concatenator.__init__\u001b[39m\u001b[34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28mself\u001b[39m.verify_integrity = verify_integrity\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m.copy = copy\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m objs, keys = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[32m    448\u001b[39m ndims = \u001b[38;5;28mself\u001b[39m._get_ndims(objs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[39m, in \u001b[36m_Concatenator._clean_keys_and_objs\u001b[39m\u001b[34m(self, objs, keys)\u001b[39m\n\u001b[32m    504\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo objects to concatenate\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    510\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(com.not_none(*objs_list))\n",
      "\u001b[31mValueError\u001b[39m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def sortRollCol(df, col):\n",
    "    digits_df = df[col].astype(str).apply(lambda x: pd.Series(list(x)))\n",
    "    \n",
    "    digits_df['original_index'] = df.index\n",
    "    \n",
    "    sorted_digits_df = recursiveSort(digits_df)\n",
    "\n",
    "    # Use the sorted indices to reorder the original dataframe\n",
    "    sorted_indices = sorted_digits_df['original_index'].values\n",
    "    sorted_df = df.loc[sorted_indices].reset_index(drop=True)\n",
    "\n",
    "    return sorted_df\n",
    "\n",
    "\n",
    "def recursiveSort(df_digits, col=0):\n",
    "    if col >= int(len(df_digits.columns) - 1):  # exclude 'original_index' column\n",
    "        return df_digits\n",
    "\n",
    "    # Sort by the current digit column\n",
    "    df_digits = df_digits.sort_values(by=col, kind='stable', ignore_index=True)\n",
    "\n",
    "    # Group by current digit and recursively sort each group\n",
    "    result = []\n",
    "    for value, group in df_digits.groupby(col, sort=False):\n",
    "        sorted_group = recursiveSort(group.reset_index(drop=True), col + 1)\n",
    "        result.append(sorted_group)\n",
    "    \n",
    "    return pd.concat(result, ignore_index=True)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "df = pd.read_csv('Files/student_dataset.csv')  # Your dataset with 'Roll No.'\n",
    "df_sorted = sortRollCol(df, )\n",
    "print(df_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "06935ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num          0\n",
      "company      0\n",
      "Type         0\n",
      "Comments    41\n",
      "dtype: int64\n",
      "Comments\n",
      "Num         0\n",
      "company     0\n",
      "Type        0\n",
      "Comments    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def dataClean(df,colTypes):\n",
    "    for i in df.index[(df.isna().sum(axis=1)==len(df.columns))]:\n",
    "        df.drop(index=i, inplace=True)\n",
    "        \n",
    "    for i in colTypes.keys():\n",
    "        if df[i].isna().sum() >=len(df[i])-2:\n",
    "            df.drop(columns=[i],inplace=True)\n",
    "            continue\n",
    "        \n",
    "        if df[i].isna().sum() != 0:#IF column has no type\n",
    "                print(i)\n",
    "                if (colTypes[i]['0'] == 60 or colTypes[i]['1'] == 40): # if column is type of year only\n",
    "                    df[i] = df[i].fillna(2100)\n",
    "                elif colTypes[i]['0'] == 30 or  colTypes[i]['1'] == 70:#if column is type of date only \n",
    "                    df[i] = df[i].fillna(pd.to_datetime('2030-01-01'))\n",
    "                elif colTypes[i]['0'] == 20 or  colTypes[i]['1'] == 80 :#if column is type of date and time \n",
    "                    df[i]= df[i].fillna('2030-01-01')\n",
    "                elif colTypes[i]['0'] == 70 or colTypes[i]['1']==30: #Roll no type\n",
    "                    df[i] = df[i].fillna(000000)\n",
    "                elif colTypes[i]['1']==50 or colTypes[i]['0']==50:#if it is of type id \n",
    "                    df[i]=df[i].fillna(\"Unknown\")\n",
    "                elif colTypes[i]['0']==90 or colTypes[i]['1'] ==10:#if it of type oneor2digit\n",
    "                    df[i] = df[i].fillna(0)\n",
    "                elif colTypes[i]['0'] == 80 or  colTypes[i]['1'] == 20 : #if it type of nemric\n",
    "                    df[i]=df[i].fillna(0.0)\n",
    "                elif colTypes[i]['0'] == 40 or  colTypes[i]['1'] == 60 : #string or object\n",
    "                    df[i] = df[i].fillna('NULL-NAN')\n",
    "        continue\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv('Files/twitter_training.csv')\n",
    "col = detectColumns(df,df.columns)\n",
    "print(df.isna().sum())\n",
    "df = dataClean(df,col)\n",
    "print(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "04fd8dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13      Rock-Hard La Varlope, RARE & POWERFUL, HANDSOM...\n",
      "20      that was the first borderlands session in a lo...\n",
      "26      The biggest disappointment of my life came a y...\n",
      "51      Blaming Sight for Tardiness! A little bit of b...\n",
      "64                                                   .. [\n",
      "                              ...                        \n",
      "4754    At the same time, despite the fact that there ...\n",
      "4755                                                  NaN\n",
      "4756                                                  NaN\n",
      "4757                                                  NaN\n",
      "4796                               buff.ly / 33Q7U7s.....\n",
      "Name: Comments, Length: 288, dtype: object\n",
      "4800\n",
      "4512\n"
     ]
    }
   ],
   "source": [
    "def FindDuplicateInStrOrOBj(series):\n",
    "    series = pd.Series(series)\n",
    "    series = series[series.duplicated()==True]\n",
    "    return series\n",
    "\n",
    "df = pd.read_csv('Files/twitter_training.csv')\n",
    "dup = FindDuplicateInStrOrOBj(df['Comments'])\n",
    "print(dup)\n",
    "print(len(df))\n",
    "df.drop_duplicates(inplace=True,subset=['Comments'])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "34b6ced4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time By Model  7.434240818023682\n",
      "Time in Normalizig : 0.002997875213623047\n",
      "Time clustering : 0.714836597442627\n",
      "Silhouette Score: 0.08340984\n",
      "Time in Normalizig : 0.04400444030761719\n",
      "    Employee_ID Task_ID  Start_Date  End_Date  login Time Logout_Time\n",
      "985       E9828  T17194  25-01-2025       NaN         NaN    18:31:11\n",
      "999       E9115  T37857  30-01-2025       NaN         NaN    19:06:08\n",
      "572       E5099  T11336  10-01-2025       NaN         NaN    18:16:00\n",
      "573       E8987  T68207  26-01-2025       NaN         NaN    18:29:35\n",
      "574       E9529  T85829  23-01-2025       NaN         NaN    18:39:01\n",
      "..          ...     ...         ...       ...         ...         ...\n",
      "969       E6790  T23862  23-12-2024       NaN         NaN    17:39:57\n",
      "37        E2356  T35740  04-01-2025       NaN         NaN    19:40:28\n",
      "34        E6234  T92956  15-01-2025       NaN         NaN    20:28:31\n",
      "31        E5396  T59696  05-12-2024       NaN         NaN    18:00:33\n",
      "25        E9638  T56134  25-12-2024       NaN         NaN    18:37:36\n",
      "\n",
      "[1000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Files/employee_time_log.csv') \n",
    "df = kmeansOnString(df,'Employee_ID')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "952da5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "def kmeansOnString(df,col):\n",
    "    phrase = (df[col]).astype(str)\n",
    "    # Step 1: Convert strings to embeddings using a Sentence Transformer (BERT-based model)\n",
    "    st = time.time()\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')  # a lightweight, efficient model\n",
    "    embeddings = model.encode(phrase)\n",
    "    et = time.time()\n",
    "    print(\"time By Model \", et-st)\n",
    "    \n",
    "    st = et\n",
    "    # Optionally, normalize embeddings if needed:\n",
    "    from sklearn.preprocessing import normalize\n",
    "    embeddings = normalize(embeddings)\n",
    "    et = time.time()\n",
    "    print(f\"Time in Normalizig : {et-st}\")\n",
    "    \n",
    "    st = et\n",
    "    # Step 2: Apply KMeans clustering on these embeddings\n",
    "    # Using n_init and max_iter helps ensure stable convergence.\n",
    "    k =  int(np.ceil(len(df[col]) / 80))# number of clusters\n",
    "    kmeans = KMeans(n_clusters=7, n_init=19, max_iter=450, random_state=42)\n",
    "    df['Cluster Id'] = kmeans.fit_predict(embeddings)\n",
    "    et = time.time()\n",
    "    print(f\"Time clustering : {et-st}\")\n",
    "    \n",
    "    st = et\n",
    "    # Step 3: Evaluate the clustering with Silhouette Score\n",
    "    score = silhouette_score(embeddings, df['Cluster Id'])\n",
    "    print(\"Silhouette Score:\", score)\n",
    "    et = time.time()\n",
    "    print(f\"Time in Normalizig : {et-st}\")\n",
    "    \n",
    "    df = df.sort_values(by='Cluster Id').drop(columns=['Cluster Id'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "91fbbf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit\n",
    "import pandas as pd\n",
    "import math\n",
    "def detectColumns(df, prioColumns):\n",
    "    result = {}    \n",
    "    for col in prioColumns:\n",
    "        qc =  QuantumCircuit(1,1)\n",
    "        col_data = df[col]\n",
    "        col_str = df[col].astype(str).str.strip()\n",
    "        # 3. Check for Date values (type 2)\n",
    "        if check_date_format(col_str):\n",
    "            p = 0.70000\n",
    "                \n",
    "        # 4. Check for DateTime values (type 3)\n",
    "        elif check_datetime_format(col_str):\n",
    "            p = 0.800000\n",
    "            \n",
    "        elif OneOr2digitDetection(col_data):\n",
    "            p = 0.100000\n",
    "        # 1. Check for Roll Numbers (type 4)\n",
    "        elif pd.api.types.is_numeric_dtype(col_data):\n",
    "            p = 0.2000\n",
    "            # 2. Check for Year values (type 1)\n",
    "            if check_year_values(col_data):\n",
    "                p = 0.40000\n",
    "            elif check_roll_number(col_data):\n",
    "                p = 0.30000\n",
    "\n",
    "        elif pd.api.types.is_string_dtype(df[col]) or pd.api.types.is_object_dtype(df[col]):\n",
    "            p = 0.600000\n",
    "            if detectIdTypeCol(col_data):\n",
    "                p = 0.5\n",
    "        \n",
    "        angle = 2 * math.asin(math.sqrt(p))\n",
    "        qc.ry(angle, 0)\n",
    "        # Initialize result as 0 (unrecognized type)\n",
    "        result[col] = measurCir(qc,0)\n",
    "        \n",
    "    return result\n",
    "\n",
    "def check_roll_number(col_data):\n",
    "    try:\n",
    "        # Convert numbers to strings for checking patterns\n",
    "        sample_start = [str(i) for i in col_data.head(10)]\n",
    "        sample_middle = [str(i) for i in col_data.iloc[int(len(col_data)/2)-5:int(len(col_data)/2)+5]]\n",
    "        sample_end = [str(i) for i in col_data.iloc[-10:]]\n",
    "        \n",
    "        # Combine samples\n",
    "        samples = sample_start + sample_middle + sample_end\n",
    "        \n",
    "        # Check if all numbers have the same length and >= 5 digits\n",
    "        if len(set(len(str(x)) for x in samples)) == 1:\n",
    "            length = len(str(samples[0]))\n",
    "            if length >= 5:\n",
    "                # Check if all numbers start with the same digit\n",
    "                first_digits = str(samples[0])[0]\n",
    "                return all(str(x).startswith(first_digit) for x in samples)\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def check_year_values(col_data):\n",
    "    if pd.api.types.is_string_dtype(col_data):\n",
    "        try:\n",
    "            col_data = pd.to_numeric(col_data)\n",
    "        except:\n",
    "            pass\n",
    "    # Handle if column is numeric and looks like a year\n",
    "    if pd.api.types.is_numeric_dtype(col_data) or  pd.api.types.is_float_dtype(col_data) or  pd.api.types.is_integer_dtype(col_data):\n",
    "        if col_data.dropna().empty == False:\n",
    "            if pd.api.types.is_float_dtype(col_data):\n",
    "            # Check if float values have only 2 decimal places\n",
    "                if col_data.dropna().apply(lambda x: round(x, 2) == x).all():\n",
    "                    if col_data.dropna().between(1800, 2050).all():\n",
    "                        return True # Only year\n",
    "            # For integer values\n",
    "            elif col_data.dropna().between(1800, 2100).all():\n",
    "                True # Only year\n",
    "                \n",
    "    return False\n",
    "\n",
    "def check_date_format(col_str):\n",
    "    # Check for common date formats (yyyy-mm-dd, dd-mm-yyyy, etc.)\n",
    "    date_pattern = r'^\\d{1,4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,4}$'\n",
    "    return col_str.str.match(date_pattern).all()\n",
    "\n",
    "def check_datetime_format(col_str):\n",
    "    # Check for datetime format (date + time)\n",
    "    datetime_pattern = r'\\d{1,4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,4}.*\\d{1,2}:\\d{2}'\n",
    "    return col_str.str.contains(datetime_pattern).all()\n",
    "\n",
    "def detectIdTypeCol(col_data):\n",
    "    if pd.api.types.is_string_dtype(col_data):\n",
    "        pattern = r'\\b[A-Z0-9]{1,4}[-_./]?[A-Z0-9]{2,6}[-_./]?[A-Z0-9]{0,5}\\b'\n",
    "        return all(re.fullmatch(pattern, item) for item in col_data)\n",
    "    \n",
    "def OneOr2digitDetection(col_data):\n",
    "    non2Digit=0\n",
    "    for v  in col_data:\n",
    "        if len(str(v)) >= 2:\n",
    "            non2Digit=o=non2Digit+1\n",
    "    \n",
    "    if len(col_data)/2.2 > non2Digit:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def detectSimpleDtypes(col_data):\n",
    "    if pd.api.types.is_integer_dtype(col_data):\n",
    "        return 'allInt'\n",
    "    if pd.api.types.is_float_dtype(col_data):\n",
    "        if col_data.isna().all()== False:\n",
    "            return 'numaric'\n",
    "    if pd.api.types.is_string_dtype(col_data) or pd.api.types.is_object_dtype(col_data):\n",
    "        return 'str'\n",
    "    return None\n",
    "\n",
    "def clusterDateTimeCol(fContent, col,no,ascending=True):\n",
    "    if no ==1:\n",
    "        # Try to detect and sort if the column is just year values\n",
    "        fContent = fContent.sort_values(by=col,ignore_index=True, ascending=ascending)\n",
    "        fContent = fContent.reset_index(drop=True)\n",
    "    elif no == 2 or 3:\n",
    "        # Now, try to detect proper date/datetime columns\n",
    "        try:\n",
    "            # Avoid processing numeric-only or zero-filled columns\n",
    "            sample_vals = fContent[col].astype(str).str.strip().replace('0', np.nan).dropna()\n",
    "            if len(sample_vals) == 0:\n",
    "                return fContent# All values are zero or empty-like\n",
    "            \n",
    "            # Try parsing\n",
    "            try:\n",
    "                parsed_col = pd.to_datetime(fContent[col], errors='raise')\n",
    "            except:\n",
    "                parsed_col = pd.to_datetime(fContent[col], dayfirst=True, errors='raise')\n",
    "               \n",
    "            if all(parsed_col.dt.time == pd.to_datetime('00:00:00').time()) and no == 2:  # Only date\n",
    "                fContent[col] = parsed_col\n",
    "                fContent = clean_and_sort_date_column(fContent, col, ascending)\n",
    "                fContent = fContent.reset_index(drop=True)\n",
    "                # Replace original column with parsed datetime values\n",
    "            elif no == 3:  # Date + time\n",
    "                fContent[col] = parsed_col\n",
    "                fContent = handle_datetime_column(fContent, col, ascending)\n",
    "                fContent = fContent.reset_index(drop=True)\n",
    "        except Exception as e:\n",
    "            return fContent # Not a datetime column\n",
    "        \n",
    "    return fContent \n",
    "\n",
    "def clean_and_sort_date_column(dff, column_name, ascending=True):\n",
    "        try:\n",
    "            \n",
    "            # Step 2: Drop NaT (invalid formats)\n",
    "            dff = dff.dropna(subset=[column_name])\n",
    "            \n",
    "            # Step 3: Sort the DataFrame by that column\n",
    "            dff = dff.sort_values(by=column_name, ascending=ascending)\n",
    "\n",
    "            # Optional: Format to clean date string (YYYY-MM-DD)\n",
    "            dff[column_name] = dff[column_name].dt.strftime('%Y-%m-%d')\n",
    "            print(f\"{column_name} date called \")\n",
    "            return dff\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error while processing date column: {e}\")\n",
    "            return dff\n",
    "\n",
    "def handle_datetime_column(df, column_name, ascending):\n",
    "    print(f\"{column_name} dateTime\")\n",
    "    # Check if most values in column are datetime with time\n",
    "    values = df[column_name].dropna().astype(str).head(20)\n",
    "    count_datetime = sum([pd.api.is_datetime(v) for v in values])\n",
    "\n",
    "    if count_datetime >= len(values) // 2:  # At least half must be datetime-like\n",
    "        # Convert full column to datetime\n",
    "        df[column_name] = pd.to_datetime(df[column_name], errors='coerce')\n",
    "        # Drop rows with invalid dates\n",
    "        df = df.dropna(subset=[column_name])\n",
    "        # Sort by that column\n",
    "        df = df.sort_values(by=column_name,ascending=ascending).reset_index(drop=True)\n",
    "        print(f\"[INFO] '{column_name}' successfully recognized and sorted as datetime.\")\n",
    "    else:\n",
    "        print(f\"[INFO] '{column_name}' does not contain proper datetime with time.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "26207e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measurCir(qc,q_num):\n",
    "    qc.measure(q_num,q_num)\n",
    "    simulator = AerSimulator()\n",
    "    # Transpile & run\n",
    "    compiled = transpile(qc, simulator)\n",
    "    r = simulator.run(compiled, shots=100000).result()\n",
    "    counts = r.get_counts()\n",
    "    for k,v in counts.items():\n",
    "        counts[k] = int(v/1000)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "0dbef6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Level 0: Sorting by column \"f1\"\n",
      "  → Cluster 0 (size 83)\n",
      "\n",
      "Level 1: Sorting by column \"f2\"\n",
      "  → Cluster 0 (size 27)\n",
      "\n",
      "Level 2: Sorting by column \"f3\"\n",
      "  → Cluster 0 (size 27)\n",
      "  ✓ Completed Cluster 0\n",
      "HII\n",
      "  ✓ Completed Cluster 0\n",
      "  → Cluster 1 (size 56)\n",
      "\n",
      "Level 2: Sorting by column \"f3\"\n",
      "  → Cluster 0 (size 56)\n",
      "  ✓ Completed Cluster 0\n",
      "HII\n",
      "  ✓ Completed Cluster 1\n",
      "HII\n",
      "  ✓ Completed Cluster 0\n",
      "  → Cluster 1 (size 73)\n",
      "\n",
      "Level 1: Sorting by column \"f2\"\n",
      "  → Cluster 0 (size 37)\n",
      "\n",
      "Level 2: Sorting by column \"f3\"\n",
      "  → Cluster 0 (size 37)\n",
      "  ✓ Completed Cluster 0\n",
      "HII\n",
      "  ✓ Completed Cluster 0\n",
      "  → Cluster 1 (size 36)\n",
      "\n",
      "Level 2: Sorting by column \"f3\"\n",
      "  → Cluster 0 (size 36)\n",
      "  ✓ Completed Cluster 0\n",
      "HII\n",
      "  ✓ Completed Cluster 1\n",
      "HII\n",
      "  ✓ Completed Cluster 1\n",
      "  → Cluster 2 (size 31)\n",
      "\n",
      "Level 1: Sorting by column \"f2\"\n",
      "  → Cluster 0 (size 31)\n",
      "\n",
      "Level 2: Sorting by column \"f3\"\n",
      "  → Cluster 0 (size 31)\n",
      "  ✓ Completed Cluster 0\n",
      "HII\n",
      "  ✓ Completed Cluster 0\n",
      "HII\n",
      "  ✓ Completed Cluster 2\n",
      "  → Cluster 3 (size 31)\n",
      "\n",
      "Level 1: Sorting by column \"f2\"\n",
      "  → Cluster 0 (size 31)\n",
      "\n",
      "Level 2: Sorting by column \"f3\"\n",
      "  → Cluster 0 (size 31)\n",
      "  ✓ Completed Cluster 0\n",
      "HII\n",
      "  ✓ Completed Cluster 0\n",
      "HII\n",
      "  ✓ Completed Cluster 3\n",
      "  → Cluster 4 (size 90)\n",
      "\n",
      "Level 1: Sorting by column \"f2\"\n",
      "  → Cluster 0 (size 62)\n",
      "\n",
      "Level 2: Sorting by column \"f3\"\n",
      "  → Cluster 0 (size 50)\n",
      "  ✓ Completed Cluster 0\n",
      "  → Cluster 1 (size 12)\n",
      "  ✓ Completed Cluster 1\n",
      "HII\n",
      "  ✓ Completed Cluster 0\n",
      "  → Cluster 1 (size 28)\n",
      "\n",
      "Level 2: Sorting by column \"f3\"\n",
      "  → Cluster 0 (size 28)\n",
      "  ✓ Completed Cluster 0\n",
      "HII\n",
      "  ✓ Completed Cluster 1\n",
      "HII\n",
      "  ✓ Completed Cluster 4\n",
      "  → Cluster 5 (size 75)\n",
      "\n",
      "Level 1: Sorting by column \"f2\"\n",
      "  → Cluster 0 (size 20)\n",
      "\n",
      "Level 2: Sorting by column \"f3\"\n",
      "  → Cluster 0 (size 20)\n",
      "  ✓ Completed Cluster 0\n",
      "HII\n",
      "  ✓ Completed Cluster 0\n",
      "  → Cluster 1 (size 55)\n",
      "\n",
      "Level 2: Sorting by column \"f3\"\n",
      "  → Cluster 0 (size 55)\n",
      "  ✓ Completed Cluster 0\n",
      "HII\n",
      "  ✓ Completed Cluster 1\n",
      "HII\n",
      "  ✓ Completed Cluster 5\n",
      "  → Cluster 6 (size 60)\n",
      "\n",
      "Level 1: Sorting by column \"f2\"\n",
      "  → Cluster 0 (size 60)\n",
      "\n",
      "Level 2: Sorting by column \"f3\"\n",
      "  → Cluster 0 (size 60)\n",
      "  ✓ Completed Cluster 0\n",
      "HII\n",
      "  ✓ Completed Cluster 0\n",
      "HII\n",
      "  ✓ Completed Cluster 6\n",
      "  → Cluster 7 (size 37)\n",
      "\n",
      "Level 1: Sorting by column \"f2\"\n",
      "  → Cluster 0 (size 37)\n",
      "\n",
      "Level 2: Sorting by column \"f3\"\n",
      "  → Cluster 0 (size 37)\n",
      "  ✓ Completed Cluster 0\n",
      "HII\n",
      "  ✓ Completed Cluster 0\n",
      "HII\n",
      "  ✓ Completed Cluster 7\n",
      "  → Cluster 8 (size 15)\n",
      "\n",
      "Level 1: Sorting by column \"f2\"\n",
      "  → Cluster 0 (size 15)\n",
      "\n",
      "Level 2: Sorting by column \"f3\"\n",
      "  → Cluster 0 (size 15)\n",
      "  ✓ Completed Cluster 0\n",
      "HII\n",
      "  ✓ Completed Cluster 0\n",
      "HII\n",
      "  ✓ Completed Cluster 8\n",
      "  → Cluster 9 (size 89)\n",
      "\n",
      "Level 1: Sorting by column \"f2\"\n",
      "  → Cluster 0 (size 34)\n",
      "\n",
      "Level 2: Sorting by column \"f3\"\n",
      "  → Cluster 0 (size 34)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[216]\u001b[39m\u001b[32m, line 158\u001b[39m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m final_sorted_df\n\u001b[32m    157\u001b[39m df = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mFiles/phpB0xrNjAllNum.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[43mcluster_based_quantum_sort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mf1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mf2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mf3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[216]\u001b[39m\u001b[32m, line 142\u001b[39m, in \u001b[36mcluster_based_quantum_sort\u001b[39m\u001b[34m(df, Pcols, n_clusters, i, order)\u001b[39m\n\u001b[32m    139\u001b[39m sorted_cluster = quantum_sort_cluster(cluster_df, sort_column)\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Recursively sort the next column (if available)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m next_sorted_cluster = \u001b[43mcluster_based_quantum_sort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43msorted_cluster\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m all_sorted.append(next_sorted_cluster)\n\u001b[32m    145\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ✓ Completed Cluster \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcluster_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[216]\u001b[39m\u001b[32m, line 142\u001b[39m, in \u001b[36mcluster_based_quantum_sort\u001b[39m\u001b[34m(df, Pcols, n_clusters, i, order)\u001b[39m\n\u001b[32m    139\u001b[39m sorted_cluster = quantum_sort_cluster(cluster_df, sort_column)\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Recursively sort the next column (if available)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m next_sorted_cluster = \u001b[43mcluster_based_quantum_sort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43msorted_cluster\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m all_sorted.append(next_sorted_cluster)\n\u001b[32m    145\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ✓ Completed Cluster \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcluster_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[216]\u001b[39m\u001b[32m, line 139\u001b[39m, in \u001b[36mcluster_based_quantum_sort\u001b[39m\u001b[34m(df, Pcols, n_clusters, i, order)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  → Cluster \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcluster_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(cluster_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# Sort this cluster\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m sorted_cluster = \u001b[43mquantum_sort_cluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_column\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Recursively sort the next column (if available)\u001b[39;00m\n\u001b[32m    142\u001b[39m next_sorted_cluster = cluster_based_quantum_sort(pd.DataFrame(sorted_cluster), Pcols, i=i+\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[216]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mquantum_sort_cluster\u001b[39m\u001b[34m(cluster_df, sort_column)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_indices:\n\u001b[32m    104\u001b[39m     remaining_values = [values[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m remaining_indices]\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     min_idx = \u001b[43mgrover_find_min_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     actual_idx = remaining_indices[min_idx]\n\u001b[32m    107\u001b[39m     sorted_indices.append(actual_idx)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[216]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mgrover_find_min_index\u001b[39m\u001b[34m(values)\u001b[39m\n\u001b[32m     85\u001b[39m     circuit.measure(qr[i], cr[i])\n\u001b[32m     87\u001b[39m backend = Aer.get_backend(\u001b[33m'\u001b[39m\u001b[33maer_simulator\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m result = \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshots\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m counts = result.get_counts()\n\u001b[32m     90\u001b[39m max_count_result = \u001b[38;5;28mmax\u001b[39m(counts.items(), key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m])[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\site-packages\\qiskit_aer\\jobs\\utils.py:38\u001b[39m, in \u001b[36mrequires_submit.<locals>._wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._future \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JobError(\u001b[33m\"\u001b[39m\u001b[33mJob not submitted yet!. You have to .submit() first!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\site-packages\\qiskit_aer\\jobs\\aerjob.py:96\u001b[39m, in \u001b[36mAerJob.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@requires_submit\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     80\u001b[39m     \u001b[38;5;66;03m# pylint: disable=arguments-differ\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get job result. The behavior is the same as the underlying\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03m    concurrent Future objects,\u001b[39;00m\n\u001b[32m     83\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m \u001b[33;03m        concurrent.futures.CancelledError: if job cancelled before completed.\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_future\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\concurrent\\futures\\_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cache for storing quantum circuits to avoid recreating them\n",
    "circuit_cache = {}\n",
    "import time\n",
    "from qiskit import QuantumRegister,ClassicalRegister\n",
    "from qiskit_aer import Aer \n",
    "\n",
    "def create_oracle(values, target_idx, num_qubits):\n",
    "    cache_key = f'oracle_{target_idx}_{num_qubits}'\n",
    "    if cache_key in circuit_cache:\n",
    "        return circuit_cache[cache_key]\n",
    "\n",
    "    oracle = QuantumCircuit(num_qubits)\n",
    "    for i in range(num_qubits):\n",
    "        if (target_idx >> i) & 1:\n",
    "            oracle.x(i)\n",
    "    \n",
    "    if num_qubits == 1:\n",
    "        oracle.h(0)\n",
    "        oracle.z(0)\n",
    "        oracle.h(0)\n",
    "    elif num_qubits > 3:\n",
    "        mid = num_qubits // 2\n",
    "        oracle.h(num_qubits - 1)\n",
    "        oracle.mcx(list(range(mid)), mid)\n",
    "        oracle.mcx(list(range(mid, num_qubits - 1)), num_qubits - 1)\n",
    "        oracle.h(num_qubits - 1)\n",
    "    else:\n",
    "        oracle.h(num_qubits - 1)\n",
    "        if num_qubits == 2:\n",
    "            oracle.cx(0, 1)\n",
    "        else:\n",
    "            oracle.mcx(list(range(num_qubits - 1)), num_qubits - 1)\n",
    "    \n",
    "    for i in range(num_qubits):\n",
    "        if (target_idx >> i) & 1:\n",
    "            oracle.x(i)\n",
    "    circuit_cache[cache_key] = oracle\n",
    "    return oracle\n",
    "\n",
    "def create_diffusion(num_qubits):\n",
    "    cache_key = f'diffusion_{num_qubits}'\n",
    "    if cache_key in circuit_cache:\n",
    "        return circuit_cache[cache_key]\n",
    "\n",
    "    diffusion = QuantumCircuit(num_qubits + 1)\n",
    "    for qubit in range(num_qubits):\n",
    "        diffusion.h(qubit)\n",
    "    for qubit in range(num_qubits):\n",
    "        diffusion.x(qubit)\n",
    "    chunk_size = 3\n",
    "    for i in range(0, num_qubits - 1, chunk_size):\n",
    "        control_qubits = list(range(i, min(i + chunk_size, num_qubits - 1)))\n",
    "        if len(control_qubits) > 0:\n",
    "            diffusion.h(num_qubits)\n",
    "            diffusion.mcx(control_qubits, num_qubits)\n",
    "            diffusion.h(num_qubits)\n",
    "    for qubit in range(num_qubits):\n",
    "        diffusion.x(qubit)\n",
    "    for qubit in range(num_qubits):\n",
    "        diffusion.h(qubit)\n",
    "    circuit_cache[cache_key] = diffusion\n",
    "    return diffusion\n",
    "\n",
    "def grover_find_min_index(values):\n",
    "    n = len(values)\n",
    "    num_bits = max(1, int(np.ceil(np.log2(n))))\n",
    "    min_idx = np.argmin(values)\n",
    "    \n",
    "    qr = QuantumRegister(num_bits + 1, 'q')\n",
    "    cr = ClassicalRegister(num_bits, 'c')\n",
    "    circuit = QuantumCircuit(qr, cr)\n",
    "    \n",
    "    for i in range(num_bits):\n",
    "        circuit.h(qr[i])\n",
    "    \n",
    "    iterations = int(np.pi/4 * np.sqrt(2**num_bits))\n",
    "    oracle = create_oracle(values, min_idx, num_bits + 1)\n",
    "    diffusion = create_diffusion(num_bits)\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        circuit = circuit.compose(oracle)\n",
    "        circuit = circuit.compose(diffusion)\n",
    "    \n",
    "    for i in range(num_bits):\n",
    "        circuit.measure(qr[i], cr[i])\n",
    "    \n",
    "    backend = Aer.get_backend('aer_simulator')\n",
    "    result = backend.run(circuit, shots=1000).result()\n",
    "    counts = result.get_counts()\n",
    "    max_count_result = max(counts.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    return int(max_count_result, 2) % n\n",
    "\n",
    "def quantum_sort_cluster(cluster_df, sort_column):\n",
    "    if len(cluster_df) == 0:\n",
    "        return cluster_df\n",
    "    \n",
    "    df = cluster_df.copy()\n",
    "    sorted_indices = []\n",
    "    values = df[sort_column].tolist()\n",
    "    remaining_indices = list(range(len(values)))\n",
    "    \n",
    "    while remaining_indices:\n",
    "        remaining_values = [values[i] for i in remaining_indices]\n",
    "        min_idx = grover_find_min_index(remaining_values)\n",
    "        actual_idx = remaining_indices[min_idx]\n",
    "        sorted_indices.append(actual_idx)\n",
    "        remaining_indices.remove(actual_idx)\n",
    "    \n",
    "    return df.iloc[sorted_indices].reset_index(drop=True)\n",
    "\n",
    "def cluster_based_quantum_sort(df, Pcols, n_clusters=None, i=0, order =True):\n",
    "    if i >= len(Pcols):\n",
    "        return df\n",
    "\n",
    "    sort_column = Pcols[i]\n",
    "    if sort_column not in df.columns:\n",
    "        print(f\"Column '{sort_column}' not found.\")\n",
    "        return df\n",
    "\n",
    "    print(f'\\nLevel {i}: Sorting by column \"{sort_column}\"')\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Perform clustering on the current sort_column\n",
    "    clustering_data = df[[sort_column]]\n",
    "    if n_clusters is None:\n",
    "        n_clusters = int(np.ceil(len(df[sort_column]) / 60))\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    df['cluster'] = kmeans.fit_predict(clustering_data)\n",
    "\n",
    "    unique_clusters = sorted(df['cluster'].unique())\n",
    "    all_sorted = []\n",
    "\n",
    "    for cluster_id in unique_clusters:\n",
    "        cluster_df = df[df['cluster'] == cluster_id].drop(columns=['cluster'])\n",
    "        print(f\"  → Cluster {cluster_id} (size {len(cluster_df)})\")\n",
    "\n",
    "        # Sort this cluster\n",
    "        sorted_cluster = quantum_sort_cluster(cluster_df, sort_column)\n",
    "\n",
    "        # Recursively sort the next column (if available)\n",
    "        next_sorted_cluster = cluster_based_quantum_sort(pd.DataFrame(sorted_cluster), Pcols, i=i+1)\n",
    "        all_sorted.append(next_sorted_cluster)\n",
    "\n",
    "        print(f\"  ✓ Completed Cluster {cluster_id}\")\n",
    "\n",
    "    # Merge and sort by current column\n",
    "    merged_df = pd.concat(all_sorted, ignore_index=True)\n",
    "    final_sorted_df = merged_df.sort_values(by=sort_column,ascending=order).reset_index(drop=True)\n",
    "\n",
    "    end_time = time.time() \n",
    "    # print(f\"✔ Level {i} sorting by '{sort_column}' completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    print(\"HII\")\n",
    "    return final_sorted_df\n",
    "\n",
    "df = pd.read_csv('Files/phpB0xrNjAllNum.csv')\n",
    "cluster_based_quantum_sort(df,['f1','f2','f3'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
