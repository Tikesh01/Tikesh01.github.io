{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5b00335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': {'0': 90, '1': 9}, 'name': {'0': 40, '1': 59}, 'properties': {'1': 59, '0': 40}}\n"
     ]
    }
   ],
   "source": [
    "#Sort Different columns: \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit_aer.primitives import Sampler\n",
    "import re\n",
    "import openpyxl\n",
    "\n",
    "df = pd.read_excel('Files/Student-1.xlsx')\n",
    "type = detectColumns(df,df.columns)\n",
    "print(type)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def digitSorting(df,col,asc):\n",
    "    df[col] = df[col].astype(str)\n",
    "    df = df.sort_values(by=col,ascending=asc, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv(\"Files/student_dataset.csv\")\n",
    "df = digitSorting(df,'Grade',asc=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1b22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error while processing date column: Can only use .dt accessor with datetimelike values\n",
      "     Index      Customer Id  First Name Last Name  \\\n",
      "39      40  BEBA4fDAA6C4adC      Rickey      Mays   \n",
      "258    259  aF03B4cFc6c05C8  Kristopher   Sanders   \n",
      "322    323  85fE9bc7A71dCB9    Kristina    Hunter   \n",
      "179    180  A5Cd45CD6FEe5A2        Seth   Osborne   \n",
      "22      23  EFeFaC727F12CDF       Glenn      Wang   \n",
      "..     ...              ...         ...       ...   \n",
      "113    114  39a6DeEEbbeF8E6       Logan    Carney   \n",
      "897    898  d268cFcCA1f1ffA     Frances     Vance   \n",
      "620    621  33a3Afa322E1033        Juan  Saunders   \n",
      "741    742  09193dD798CFB29     Phillip     Duffy   \n",
      "694    695  E51a2f6ab11a5cf      Norman    Morton   \n",
      "\n",
      "                          Company               City  \\\n",
      "39    Escobar, Carrillo and Sloan       Hollandshire   \n",
      "258                 Lamb-Oconnell    North Judyville   \n",
      "322                  Fry-Melendez         East Isaac   \n",
      "179                Rollins-Carson        East Kaylee   \n",
      "22                   Warner-Hodge       West Rachael   \n",
      "..                            ...                ...   \n",
      "113               Jensen-Crawford           Omarport   \n",
      "897      Montoya, Munoz and Riggs  North Evelynmouth   \n",
      "620                 Gillespie Inc           Sotoberg   \n",
      "741  Booker, Ritter and Daugherty         Lake Emily   \n",
      "694   Vaughn, Beasley and Holland  West Vanessahaven   \n",
      "\n",
      "                           Country                 Phone 1  \\\n",
      "39        United States of America      042-976-4714x26341   \n",
      "258                        Comoros    001-610-597-9134x568   \n",
      "322                        Ecuador       414.387.9962x8086   \n",
      "179  Holy See (Vatican City State)            423-915-1795   \n",
      "22                           Gabon       834.104.6424x8311   \n",
      "..                             ...                     ...   \n",
      "113                      Gibraltar              5052283490   \n",
      "897                        Ukraine  001-078-423-6098x72526   \n",
      "620                         Taiwan            622.669.1603   \n",
      "741                    Saint Lucia              7397048829   \n",
      "694                     Cape Verde           (202)135-7697   \n",
      "\n",
      "                   Phone 2                          Email Subscription Date  \\\n",
      "39            245.657.5660       cmcdowell@riley-wolf.org        01-01-2020   \n",
      "258      (299)038-9462x724               eperry@yates.com        01-01-2021   \n",
      "322       275.749.8108x662      bonnie63@potter-combs.com        01-01-2022   \n",
      "179           848-710-5884   xcoleman@farrell-bernard.com        01-01-2022   \n",
      "22        001-741-628-9295                anna80@mata.com        01-01-2022   \n",
      "..                     ...                            ...               ...   \n",
      "113     (403)691-1260x8600          rebecca89@marquez.net        31-10-2020   \n",
      "897  001-153-923-1964x7271               rlutz@dodson.org        31-10-2021   \n",
      "620       588.688.6058x638              choidean@hays.com        31-12-2020   \n",
      "741      611-287-1073x1735           christian65@mayo.com        31-12-2020   \n",
      "694           892.100.1608  jermaine75@fowler-hancock.com        31-12-2021   \n",
      "\n",
      "                            Website  \n",
      "39            http://www.nolan.com/  \n",
      "258        https://www.warner.info/  \n",
      "322              http://jensen.com/  \n",
      "179  http://www.bradford-rivas.com/  \n",
      "22          http://brooks-kerr.com/  \n",
      "..                              ...  \n",
      "113           http://www.giles.com/  \n",
      "897         https://www.galvan.com/  \n",
      "620         https://www.coffey.com/  \n",
      "741          http://www.conner.net/  \n",
      "694         https://www.hudson.com/  \n",
      "\n",
      "[1000 rows x 12 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "time data \"13-02-2020\" doesn't match format \"%m-%d-%Y\", at position 245. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[32m     45\u001b[39m \u001b[38;5;28mtype\u001b[39m = {\n\u001b[32m     46\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m0\u001b[39m\u001b[33m'\u001b[39m : \u001b[32m30\u001b[39m, \n\u001b[32m     47\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m : \u001b[32m70\u001b[39m\n\u001b[32m     48\u001b[39m         }\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m df = \u001b[43mmultiIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolToCheck\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSubscription Date\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcolType\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43masc\u001b[49m\u001b[43m=\u001b[49m\u001b[43masc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m df\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mmultiIndex\u001b[39m\u001b[34m(dataFrame, colToCheck, colType, yearOnly, asc)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (colType[\u001b[33m'\u001b[39m\u001b[33m0\u001b[39m\u001b[33m'\u001b[39m] == \u001b[32m30\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m  colType[\u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m] == \u001b[32m70\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (colType[\u001b[33m'\u001b[39m\u001b[33m0\u001b[39m\u001b[33m'\u001b[39m] == \u001b[32m20\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m  colType[\u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m] == \u001b[32m80\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m         yearsWithLastIndex = get_last_indices_of_each_year(\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataFrame\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolToCheck\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m(colType[\u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m]==\u001b[32m30\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m colType[\u001b[33m'\u001b[39m\u001b[33m0\u001b[39m\u001b[33m'\u001b[39m]==\u001b[32m70\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (colType[\u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m] ==\u001b[32m50\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m colType[\u001b[33m'\u001b[39m\u001b[33m0\u001b[39m\u001b[33m'\u001b[39m]==\u001b[32m50\u001b[39m):\n\u001b[32m      9\u001b[39m         yearsWithLastIndex = get_last_indices_of_each_year(dataFrame[colToCheck],\u001b[38;5;28;01mFalse\u001b[39;00m,\u001b[38;5;28;01mTrue\u001b[39;00m,asc=asc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1063\u001b[39m, in \u001b[36mto_datetime\u001b[39m\u001b[34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[39m\n\u001b[32m   1061\u001b[39m             result = arg.tz_localize(\u001b[33m\"\u001b[39m\u001b[33mutc\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1062\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m     cache_array = \u001b[43m_maybe_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_listlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array.empty:\n\u001b[32m   1065\u001b[39m         result = arg.map(cache_array)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:247\u001b[39m, in \u001b[36m_maybe_cache\u001b[39m\u001b[34m(arg, format, cache, convert_listlike)\u001b[39m\n\u001b[32m    245\u001b[39m unique_dates = unique(arg)\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) < \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     cache_dates = \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:433\u001b[39m, in \u001b[36m_convert_listlike_datetimes\u001b[39m\u001b[34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33mmixed\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    435\u001b[39m result, tz_parsed = objects_to_datetime64(\n\u001b[32m    436\u001b[39m     arg,\n\u001b[32m    437\u001b[39m     dayfirst=dayfirst,\n\u001b[32m   (...)\u001b[39m\u001b[32m    441\u001b[39m     allow_object=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    442\u001b[39m )\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    445\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m    446\u001b[39m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:467\u001b[39m, in \u001b[36m_array_strptime_with_fallback\u001b[39m\u001b[34m(arg, name, utc, fmt, exact, errors)\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_array_strptime_with_fallback\u001b[39m(\n\u001b[32m    457\u001b[39m     arg,\n\u001b[32m    458\u001b[39m     name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    462\u001b[39m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    463\u001b[39m ) -> Index:\n\u001b[32m    464\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[33;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m     result, tz_out = \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    468\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    469\u001b[39m         unit = np.datetime_data(result.dtype)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mstrptime.pyx:501\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime.array_strptime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mstrptime.pyx:451\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime.array_strptime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mstrptime.pyx:583\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime._parse_with_format\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: time data \"13-02-2020\" doesn't match format \"%m-%d-%Y\", at position 245. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def sortRollCol(df, col,asc):\n",
    "    digits_df = df[col].astype(str).apply(lambda x: pd.Series(list(x)))\n",
    "    \n",
    "    digits_df['original_index'] = df.index\n",
    "    \n",
    "    sorted_digits_df = recursiveSort(digits_df)\n",
    "\n",
    "    sorted_indices = sorted_digits_df['original_index'].values\n",
    "    if asc == False:\n",
    "        sorted_indices = reversed(sorted_indices)\n",
    "        \n",
    "    sorted_df = df.loc[sorted_indices].reset_index(drop=True)\n",
    "\n",
    "    return sorted_df\n",
    "\n",
    "\n",
    "def recursiveSort(df_digits, col=0):\n",
    "    if col >= int(len(df_digits.columns) - 1):  # exclude 'original_index' column\n",
    "        return df_digits\n",
    "\n",
    "    # Sort by the current digit column\n",
    "    df_digits = df_digits.sort_values(by=col, kind='stable', ignore_index=True)\n",
    "\n",
    "    # Group by current digit and recursively sort each group\n",
    "    result = []\n",
    "    for value, group in df_digits.groupby(col, sort=False):\n",
    "        sorted_group = recursiveSort(group.reset_index(drop=True), col + 1)\n",
    "        result.append(sorted_group)\n",
    "    \n",
    "    return pd.concat(result, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06935ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataClean(df,colTypes):\n",
    "    for i in df.index[(df.isna().sum(axis=1)==len(df.columns))]:\n",
    "        df.drop(index=i, inplace=True)\n",
    "        \n",
    "    for i in colTypes.keys():\n",
    "        if df[i].isna().sum() >=len(df[i])-2:\n",
    "            df.drop(columns=[i],inplace=True)\n",
    "            continue\n",
    "        \n",
    "        if df[i].isna().sum() != 0:#IF column has no type\n",
    "                print(i)\n",
    "                if (colTypes[i]['0'] == 60 or colTypes[i]['1'] == 40): # if column is type of year only\n",
    "                    df[i] = df[i].fillna(2100)\n",
    "                elif colTypes[i]['0'] == 30 or  colTypes[i]['1'] == 70:#if column is type of date only \n",
    "                    df[i] = df[i].fillna(pd.to_datetime('2030-01-01'))\n",
    "                elif colTypes[i]['0'] == 20 or  colTypes[i]['1'] == 80 :#if column is type of date and time \n",
    "                    df[i]= df[i].fillna('2030-01-01')\n",
    "                elif colTypes[i]['0'] == 70 or colTypes[i]['1']==30: #Roll no type\n",
    "                    pass\n",
    "                elif colTypes[i]['1']==50 or colTypes[i]['0']==50:#if it is of type id \n",
    "                    df[i]=df[i].fillna(\"Unknown\")\n",
    "                elif colTypes[i]['0']==90 or colTypes[i]['1'] ==10:#if it of type oneor2digit\n",
    "                    df[i] = df[i].fillna(0)\n",
    "                elif colTypes[i]['0'] == 80 or  colTypes[i]['1'] == 20 : #if it type of nemric\n",
    "                    df[i]=df[i].fillna(0.0)\n",
    "                elif colTypes[i]['0'] == 40 or  colTypes[i]['1'] == 60 : #string or object\n",
    "                    df[i] = df[i].fillna('NAN')\n",
    "        continue\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv('Files/business-financial-data-december-2024-quarter-csv.csv')\n",
    "col = detectColumns(df,df.columns)\n",
    "print(df.isna().sum())\n",
    "df = dataClean(df,col)\n",
    "print(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd8dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDuplicate(series):\n",
    "    series = series[series.duplicated(keep=False)]\n",
    "    return series\n",
    "\n",
    "df = pd.read_csv('Files/twitter_training.csv')\n",
    "dup = findDuplicate(df['Comments'])\n",
    "print(dup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b6ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Files/employee_time_log.csv') \n",
    "df = kmeansOnString(df,'Employee_ID')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde259db",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['bdca.930-y','bdca.uei-9','ueii/783-77']\n",
    "df = pd.DataFrame(x)\n",
    "for i,v in enumerate(df[0]):\n",
    "    df[0][i] = df[0][i][:df[0][i].find('.')]+df[0][i][df[0][i].find('.')+1:]\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "952da5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "def kmeansOnString(df,col):\n",
    "    phrase = (df[col]).astype(str)\n",
    "    # Step 1: Convert strings to embeddings using a Sentence Transformer (BERT-based model)\n",
    "    st = time.time()\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')  # a lightweight, efficient model\n",
    "    embeddings = model.encode(phrase)\n",
    "    et = time.time()\n",
    "    print(\"time By Model \", et-st)\n",
    "    \n",
    "    st = et\n",
    "    # Optionally, normalize embeddings if needed:\n",
    "    from sklearn.preprocessing import normalize\n",
    "    embeddings = normalize(embeddings)\n",
    "    et = time.time()\n",
    "    print(f\"Time in Normalizig : {et-st}\")\n",
    "    \n",
    "    st = et\n",
    "    # Step 2: Apply KMeans clustering on these embeddings\n",
    "    # Using n_init and max_iter helps ensure stable convergence.\n",
    "    k =  int(np.ceil(len(df[col]) / 80))# number of clusters\n",
    "    kmeans = KMeans(n_clusters=7, n_init=19, max_iter=450, random_state=42)\n",
    "    df['Cluster Id'] = kmeans.fit_predict(embeddings)\n",
    "    et = time.time()\n",
    "    print(f\"Time clustering : {et-st}\")\n",
    "    \n",
    "    st = et\n",
    "    # Step 3: Evaluate the clustering with Silhouette Score\n",
    "    score = silhouette_score(embeddings, df['Cluster Id'])\n",
    "    print(\"Silhouette Score:\", score)\n",
    "    et = time.time()\n",
    "    print(f\"Time in Normalizig : {et-st}\")\n",
    "    \n",
    "    df = df.sort_values(by='Cluster Id').drop(columns=['Cluster Id'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8294a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiIndex(dataFrame, colToCheck, colType, yearOnly = False, asc=True):\n",
    "    # Step 1: Get last indices of each year\n",
    "    if yearOnly == True:\n",
    "        yearsWithLastIndex = get_last_indices_of_each_year(dataFrame[colToCheck], True)\n",
    "    else: \n",
    "        if (colType['0'] == 30 or  colType['1'] == 70) or (colType['0'] == 20 or  colType['1'] == 80):\n",
    "            yearsWithLastIndex = get_last_indices_of_each_year(pd.to_datetime(dataFrame[colToCheck]),False)\n",
    "        elif(colType['1']==30 or colType['0']==70) or (colType['1'] ==50 or colType['0']==50):\n",
    "            yearsWithLastIndex = get_last_indices_of_each_year(dataFrame[colToCheck],False,True,asc=asc)\n",
    "            asc=True\n",
    "        else:   \n",
    "            yearsWithLastIndex = get_last_indices_of_each_year(dataFrame[colToCheck], False)\n",
    "    if asc==False:\n",
    "        yearsWithLastIndex = dict(reversed(list(yearsWithLastIndex.items())))\n",
    "        \n",
    "    nameOfGroups = list(yearsWithLastIndex.keys())\n",
    "    last_indices = list(yearsWithLastIndex.values())\n",
    "\n",
    "    # Step 2: Compute counts from last indices\n",
    "    group_sizes = []\n",
    "    prev = -1\n",
    "    for idx in last_indices:\n",
    "        group_sizes.append(idx - prev)\n",
    "        prev = idx\n",
    "    print(group_sizes)\n",
    "    # Step 3: Create array per group\n",
    "    objOfGroups = {\n",
    "        f'key{i}': np.array([f'Group of {year}'] * group_sizes[i]) for i, year in enumerate(nameOfGroups)\n",
    "    }\n",
    "    \n",
    "    # Step 4: Combine into one array\n",
    "    outside = np.concatenate(list(objOfGroups.values()))\n",
    "    inside = np.arange(len(outside))\n",
    "        \n",
    "    print(outside)\n",
    "    # Step 5: Create inside index\n",
    "    \n",
    "    multi_index = pd.MultiIndex.from_arrays([outside, inside], names=[\"Group\", \"Sr No.\"])\n",
    "    \n",
    "    dataFrame = dataFrame.reindex(range(len(multi_index)))\n",
    "    dataFrame.set_index(multi_index,inplace=True,)\n",
    "    print(\"Multicalled\")\n",
    "    return dataFrame\n",
    "\n",
    "def get_last_indices_of_each_year(date_series, YearOnly=False,rol=False, a=0.60,asc=True):\n",
    "    \n",
    "    # data_series = data_series.apply(pd.to_numeric, errors='coerce').astype('Int64')\n",
    "    if rol==True and findDuplicate(date_series).count() <=8:\n",
    "            df = pd.DataFrame({'year':date_series},index=np.arange(len(date_series))).astype(str)\n",
    "            df['half'] = df['year'].apply(lambda x: x[:round(len(str(x))*a)])\n",
    "            print(a)\n",
    "            last_indices =  df.groupby('half',).apply(lambda x: x.index[-1]).to_dict()\n",
    "            if asc ==False:\n",
    "                last_indices = dict(reversed(list(last_indices.items())))\n",
    "                \n",
    "            group_sizes = []\n",
    "            prev = -1\n",
    "            k=0\n",
    "            for i,idx in enumerate(last_indices.values()):\n",
    "                group_sizes.append(idx - prev)\n",
    "                prev = idx\n",
    "                if group_sizes[i]<= 15:\n",
    "                    k=k+1\n",
    "            if int(len(last_indices)*0.40)<=k and a>=0.10:\n",
    "                last_indices = get_last_indices_of_each_year(date_series,False,True,a=a-0.10, asc=asc)\n",
    "            return last_indices\n",
    "    if YearOnly == True:\n",
    "        print(\"yearOnly work\")\n",
    "        df = pd.DataFrame({'year': date_series}, index=np.arange(len(date_series)))\n",
    "    \n",
    "    else:\n",
    "        # Extract year\n",
    "        try:\n",
    "            years = date_series.dt.year\n",
    "            # Create a DataFrame with index\n",
    "            df = pd.DataFrame({'year': years}, index=np.arange(len(date_series)))\n",
    "            print(\"Year Extracted\")\n",
    "        except Exception as e:\n",
    "            #Create a DataFrame with index\n",
    "            print(e)\n",
    "            df = pd.DataFrame({'year': date_series}, index=np.arange(len(date_series)))\n",
    "        \n",
    "    # Get last index of each year group\n",
    "    last_indices = df.groupby('year').apply(lambda x: x.index[-1]).to_dict()\n",
    "    print(\"last index called\")\n",
    "    return last_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1392d60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try\n",
      "Subscription Date date called \n",
      "     Index      Customer Id First Name Last Name                      Company  \\\n",
      "0       40  BEBA4fDAA6C4adC     Rickey      Mays  Escobar, Carrillo and Sloan   \n",
      "1      242  dEc837d5F13C1ed     Kendra    Waters                    Gates Inc   \n",
      "2      617  F9AEc2F51C5EDaE     Parker     Russo                  Foley-Yoder   \n",
      "3      148  EF5858dEe5f7649    Belinda  Ferguson      Lewis, Bowman and Craig   \n",
      "4      952  a9feB7dCD2FFd4e      Larry       Key                  Riley Group   \n",
      "..     ...              ...        ...       ...                          ...   \n",
      "995    772  cDBe5cFdbB3Ae4F     Jasmin    Waters                Chandler-Holt   \n",
      "996    831  4A231C5DceB9739       Jack   Mendoza  Cardenas, Bass and Callahan   \n",
      "997    498  E040edB499A6132     Amanda    Santos                 Camacho-Lamb   \n",
      "998    198  a8FfE4fbd7910b9    Bethany   Barrera  Swanson, Figueroa and Heath   \n",
      "999    578  aF4fA3aCA4bD5eC       Joel      Shea               Richmond-Horne   \n",
      "\n",
      "                    City                                       Country  \\\n",
      "0           Hollandshire                      United States of America   \n",
      "1             Warnerport                                     Nicaragua   \n",
      "2           East Dorothy                                        Malawi   \n",
      "3            Moralesport              Lao People's Democratic Republic   \n",
      "4        Mcdonaldchester                                        Guinea   \n",
      "..                   ...                                           ...   \n",
      "995  South Marisachester                                       Hungary   \n",
      "996            Quinnfurt                                   Puerto Rico   \n",
      "997          Freemanberg                           Antigua and Barbuda   \n",
      "998           Vickietown  South Georgia and the South Sandwich Islands   \n",
      "999         South Alisha                                         Palau   \n",
      "\n",
      "                   Phone 1                Phone 2  \\\n",
      "0       042-976-4714x26341           245.657.5660   \n",
      "1               5445638365           939.571.9576   \n",
      "2             800-134-7296      844-503-8567x9308   \n",
      "3             307.998.0543           007.052.7419   \n",
      "4     001-406-973-8446x255           811.758.6793   \n",
      "..                     ...                    ...   \n",
      "995    (100)042-3614x67556             6319120275   \n",
      "996  +1-186-954-2345x50800  001-265-899-4876x1796   \n",
      "997      092.983.8391x0219     626-158-4763x92618   \n",
      "998       001-411-057-3486             6232251109   \n",
      "999  +1-517-016-8892x66533    (060)659-5698x44574   \n",
      "\n",
      "                            Email Subscription Date  \\\n",
      "0        cmcdowell@riley-wolf.org        2020-01-01   \n",
      "1             tracey11@carney.com        2020-01-02   \n",
      "2       hancockbrianna@mccann.org        2020-01-02   \n",
      "3           billspears@harmon.org        2020-01-02   \n",
      "4                ygood@vaughn.com        2020-01-04   \n",
      "..                            ...               ...   \n",
      "995             pclark@ortega.com        2022-05-26   \n",
      "996        kmccullough@bryant.com        2022-05-28   \n",
      "997  slivingston@cherry-lara.info        2022-05-29   \n",
      "998          rhonda48@castro.info        2022-05-29   \n",
      "999     gfarley@wheeler-ayala.com        2022-05-29   \n",
      "\n",
      "                             Website  \n",
      "0              http://www.nolan.com/  \n",
      "1              http://www.ayala.com/  \n",
      "2         https://perez-pollard.com/  \n",
      "3                  https://huff.com/  \n",
      "4    https://www.arroyo-schultz.com/  \n",
      "..                               ...  \n",
      "995         https://costa-owens.com/  \n",
      "996               https://colon.net/  \n",
      "997     http://www.mcneil-gould.biz/  \n",
      "998           http://www.cortez.com/  \n",
      "999          https://www.mercer.com/  \n",
      "\n",
      "[1000 rows x 12 columns]\n",
      "Year Extracted\n",
      "last index called\n",
      "[426, 404, 170]\n",
      "['Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2020' 'Group of 2020'\n",
      " 'Group of 2020' 'Group of 2020' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2021' 'Group of 2021'\n",
      " 'Group of 2021' 'Group of 2021' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022'\n",
      " 'Group of 2022' 'Group of 2022' 'Group of 2022' 'Group of 2022']\n",
      "Multicalled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tikes\\AppData\\Local\\Temp\\ipykernel_10752\\3254328685.py:131: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  parsed_col = pd.to_datetime(fContent[col], errors='raise')\n",
      "C:\\Users\\tikes\\AppData\\Local\\Temp\\ipykernel_10752\\3785960857.py:84: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  last_indices = df.groupby('year').apply(lambda x: x.index[-1]).to_dict()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Customer Id</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Company</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Phone 1</th>\n",
       "      <th>Phone 2</th>\n",
       "      <th>Email</th>\n",
       "      <th>Subscription Date</th>\n",
       "      <th>Website</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Group</th>\n",
       "      <th>Sr No.</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Group of 2020</th>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>BEBA4fDAA6C4adC</td>\n",
       "      <td>Rickey</td>\n",
       "      <td>Mays</td>\n",
       "      <td>Escobar, Carrillo and Sloan</td>\n",
       "      <td>Hollandshire</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>042-976-4714x26341</td>\n",
       "      <td>245.657.5660</td>\n",
       "      <td>cmcdowell@riley-wolf.org</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>http://www.nolan.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>242</td>\n",
       "      <td>dEc837d5F13C1ed</td>\n",
       "      <td>Kendra</td>\n",
       "      <td>Waters</td>\n",
       "      <td>Gates Inc</td>\n",
       "      <td>Warnerport</td>\n",
       "      <td>Nicaragua</td>\n",
       "      <td>5445638365</td>\n",
       "      <td>939.571.9576</td>\n",
       "      <td>tracey11@carney.com</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>http://www.ayala.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>617</td>\n",
       "      <td>F9AEc2F51C5EDaE</td>\n",
       "      <td>Parker</td>\n",
       "      <td>Russo</td>\n",
       "      <td>Foley-Yoder</td>\n",
       "      <td>East Dorothy</td>\n",
       "      <td>Malawi</td>\n",
       "      <td>800-134-7296</td>\n",
       "      <td>844-503-8567x9308</td>\n",
       "      <td>hancockbrianna@mccann.org</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>https://perez-pollard.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148</td>\n",
       "      <td>EF5858dEe5f7649</td>\n",
       "      <td>Belinda</td>\n",
       "      <td>Ferguson</td>\n",
       "      <td>Lewis, Bowman and Craig</td>\n",
       "      <td>Moralesport</td>\n",
       "      <td>Lao People's Democratic Republic</td>\n",
       "      <td>307.998.0543</td>\n",
       "      <td>007.052.7419</td>\n",
       "      <td>billspears@harmon.org</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>https://huff.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>952</td>\n",
       "      <td>a9feB7dCD2FFd4e</td>\n",
       "      <td>Larry</td>\n",
       "      <td>Key</td>\n",
       "      <td>Riley Group</td>\n",
       "      <td>Mcdonaldchester</td>\n",
       "      <td>Guinea</td>\n",
       "      <td>001-406-973-8446x255</td>\n",
       "      <td>811.758.6793</td>\n",
       "      <td>ygood@vaughn.com</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>https://www.arroyo-schultz.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Group of 2022</th>\n",
       "      <th>995</th>\n",
       "      <td>772</td>\n",
       "      <td>cDBe5cFdbB3Ae4F</td>\n",
       "      <td>Jasmin</td>\n",
       "      <td>Waters</td>\n",
       "      <td>Chandler-Holt</td>\n",
       "      <td>South Marisachester</td>\n",
       "      <td>Hungary</td>\n",
       "      <td>(100)042-3614x67556</td>\n",
       "      <td>6319120275</td>\n",
       "      <td>pclark@ortega.com</td>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>https://costa-owens.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>831</td>\n",
       "      <td>4A231C5DceB9739</td>\n",
       "      <td>Jack</td>\n",
       "      <td>Mendoza</td>\n",
       "      <td>Cardenas, Bass and Callahan</td>\n",
       "      <td>Quinnfurt</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>+1-186-954-2345x50800</td>\n",
       "      <td>001-265-899-4876x1796</td>\n",
       "      <td>kmccullough@bryant.com</td>\n",
       "      <td>2022-05-28</td>\n",
       "      <td>https://colon.net/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>498</td>\n",
       "      <td>E040edB499A6132</td>\n",
       "      <td>Amanda</td>\n",
       "      <td>Santos</td>\n",
       "      <td>Camacho-Lamb</td>\n",
       "      <td>Freemanberg</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>092.983.8391x0219</td>\n",
       "      <td>626-158-4763x92618</td>\n",
       "      <td>slivingston@cherry-lara.info</td>\n",
       "      <td>2022-05-29</td>\n",
       "      <td>http://www.mcneil-gould.biz/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>198</td>\n",
       "      <td>a8FfE4fbd7910b9</td>\n",
       "      <td>Bethany</td>\n",
       "      <td>Barrera</td>\n",
       "      <td>Swanson, Figueroa and Heath</td>\n",
       "      <td>Vickietown</td>\n",
       "      <td>South Georgia and the South Sandwich Islands</td>\n",
       "      <td>001-411-057-3486</td>\n",
       "      <td>6232251109</td>\n",
       "      <td>rhonda48@castro.info</td>\n",
       "      <td>2022-05-29</td>\n",
       "      <td>http://www.cortez.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>578</td>\n",
       "      <td>aF4fA3aCA4bD5eC</td>\n",
       "      <td>Joel</td>\n",
       "      <td>Shea</td>\n",
       "      <td>Richmond-Horne</td>\n",
       "      <td>South Alisha</td>\n",
       "      <td>Palau</td>\n",
       "      <td>+1-517-016-8892x66533</td>\n",
       "      <td>(060)659-5698x44574</td>\n",
       "      <td>gfarley@wheeler-ayala.com</td>\n",
       "      <td>2022-05-29</td>\n",
       "      <td>https://www.mercer.com/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Index      Customer Id First Name Last Name  \\\n",
       "Group         Sr No.                                                \n",
       "Group of 2020 0          40  BEBA4fDAA6C4adC     Rickey      Mays   \n",
       "              1         242  dEc837d5F13C1ed     Kendra    Waters   \n",
       "              2         617  F9AEc2F51C5EDaE     Parker     Russo   \n",
       "              3         148  EF5858dEe5f7649    Belinda  Ferguson   \n",
       "              4         952  a9feB7dCD2FFd4e      Larry       Key   \n",
       "...                     ...              ...        ...       ...   \n",
       "Group of 2022 995       772  cDBe5cFdbB3Ae4F     Jasmin    Waters   \n",
       "              996       831  4A231C5DceB9739       Jack   Mendoza   \n",
       "              997       498  E040edB499A6132     Amanda    Santos   \n",
       "              998       198  a8FfE4fbd7910b9    Bethany   Barrera   \n",
       "              999       578  aF4fA3aCA4bD5eC       Joel      Shea   \n",
       "\n",
       "                                          Company                 City  \\\n",
       "Group         Sr No.                                                     \n",
       "Group of 2020 0       Escobar, Carrillo and Sloan         Hollandshire   \n",
       "              1                         Gates Inc           Warnerport   \n",
       "              2                       Foley-Yoder         East Dorothy   \n",
       "              3           Lewis, Bowman and Craig          Moralesport   \n",
       "              4                       Riley Group      Mcdonaldchester   \n",
       "...                                           ...                  ...   \n",
       "Group of 2022 995                   Chandler-Holt  South Marisachester   \n",
       "              996     Cardenas, Bass and Callahan            Quinnfurt   \n",
       "              997                    Camacho-Lamb          Freemanberg   \n",
       "              998     Swanson, Figueroa and Heath           Vickietown   \n",
       "              999                  Richmond-Horne         South Alisha   \n",
       "\n",
       "                                                           Country  \\\n",
       "Group         Sr No.                                                 \n",
       "Group of 2020 0                           United States of America   \n",
       "              1                                          Nicaragua   \n",
       "              2                                             Malawi   \n",
       "              3                   Lao People's Democratic Republic   \n",
       "              4                                             Guinea   \n",
       "...                                                            ...   \n",
       "Group of 2022 995                                          Hungary   \n",
       "              996                                      Puerto Rico   \n",
       "              997                              Antigua and Barbuda   \n",
       "              998     South Georgia and the South Sandwich Islands   \n",
       "              999                                            Palau   \n",
       "\n",
       "                                    Phone 1                Phone 2  \\\n",
       "Group         Sr No.                                                 \n",
       "Group of 2020 0          042-976-4714x26341           245.657.5660   \n",
       "              1                  5445638365           939.571.9576   \n",
       "              2                800-134-7296      844-503-8567x9308   \n",
       "              3                307.998.0543           007.052.7419   \n",
       "              4        001-406-973-8446x255           811.758.6793   \n",
       "...                                     ...                    ...   \n",
       "Group of 2022 995       (100)042-3614x67556             6319120275   \n",
       "              996     +1-186-954-2345x50800  001-265-899-4876x1796   \n",
       "              997         092.983.8391x0219     626-158-4763x92618   \n",
       "              998          001-411-057-3486             6232251109   \n",
       "              999     +1-517-016-8892x66533    (060)659-5698x44574   \n",
       "\n",
       "                                             Email Subscription Date  \\\n",
       "Group         Sr No.                                                   \n",
       "Group of 2020 0           cmcdowell@riley-wolf.org        2020-01-01   \n",
       "              1                tracey11@carney.com        2020-01-02   \n",
       "              2          hancockbrianna@mccann.org        2020-01-02   \n",
       "              3              billspears@harmon.org        2020-01-02   \n",
       "              4                   ygood@vaughn.com        2020-01-04   \n",
       "...                                            ...               ...   \n",
       "Group of 2022 995                pclark@ortega.com        2022-05-26   \n",
       "              996           kmccullough@bryant.com        2022-05-28   \n",
       "              997     slivingston@cherry-lara.info        2022-05-29   \n",
       "              998             rhonda48@castro.info        2022-05-29   \n",
       "              999        gfarley@wheeler-ayala.com        2022-05-29   \n",
       "\n",
       "                                              Website  \n",
       "Group         Sr No.                                   \n",
       "Group of 2020 0                 http://www.nolan.com/  \n",
       "              1                 http://www.ayala.com/  \n",
       "              2            https://perez-pollard.com/  \n",
       "              3                     https://huff.com/  \n",
       "              4       https://www.arroyo-schultz.com/  \n",
       "...                                               ...  \n",
       "Group of 2022 995            https://costa-owens.com/  \n",
       "              996                  https://colon.net/  \n",
       "              997        http://www.mcneil-gould.biz/  \n",
       "              998              http://www.cortez.com/  \n",
       "              999             https://www.mercer.com/  \n",
       "\n",
       "[1000 rows x 12 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "df = pd.read_csv('Files/customers-1000.csv')  # Your dataset with 'Roll No.'\n",
    "asc = True\n",
    "# df = sortRollCol(df,'',asc=asc)\n",
    "df = clusterDateTimeCol(df,'Subscription Date',2, asc)\n",
    "print(df)\n",
    "type = {\n",
    "        '0' : 30, \n",
    "        '1' : 70\n",
    "        }\n",
    "df = multiIndex(df,colToCheck='Subscription Date',colType=type,asc=asc)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91fbbf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit\n",
    "import pandas as pd\n",
    "import math\n",
    "def detectColumns(df, prioColumns):\n",
    "    result = {}    \n",
    "    for col in prioColumns:\n",
    "        qc =  QuantumCircuit(1,1)\n",
    "        col_data = df[col]\n",
    "        col_str = df[col].astype(str).str.strip()\n",
    "        # 3. Check for Date values (type 2)\n",
    "        if check_date_format(col_str) :\n",
    "            p = 0.70000\n",
    "                \n",
    "        # 4. Check for DateTime values (type 3)\n",
    "        elif check_datetime_format(col_str):\n",
    "            p = 0.800000\n",
    "            \n",
    "        elif OneOr2digitDetection(col_data):\n",
    "            p = 0.100000\n",
    "        # 1. Check for Roll Numbers (type 4)\n",
    "        elif pd.api.types.is_numeric_dtype(col_data):\n",
    "            p = 0.2000\n",
    "            # 2. Check for Year values (type 1)\n",
    "            if check_year_values(col_data):\n",
    "                p = 0.40000\n",
    "            elif check_roll_number(col_data):\n",
    "                p = 0.30000\n",
    "\n",
    "        elif pd.api.types.is_string_dtype(df[col]) or pd.api.types.is_object_dtype(df[col]):\n",
    "            p = 0.600000\n",
    "            if detectIdTypeCol(col_data):\n",
    "                p = 0.5\n",
    "        \n",
    "        angle = 2 * math.asin(math.sqrt(p))\n",
    "        qc.ry(angle, 0)\n",
    "        # Initialize result as 0 (unrecognized type)\n",
    "        result[col] = measurCir(qc,0)\n",
    "        \n",
    "    return result\n",
    "\n",
    "def check_roll_number(col_data):\n",
    "    try:\n",
    "        # Convert numbers to strings for checking patterns\n",
    "        sample_start = [str(i) for i in col_data.head(10)]\n",
    "        sample_middle = [str(i) for i in col_data.iloc[int(len(col_data)/2)-5:int(len(col_data)/2)+5]]\n",
    "        sample_end = [str(i) for i in col_data.iloc[-10:]]\n",
    "        \n",
    "        # Combine samples\n",
    "        samples = sample_start + sample_middle + sample_end\n",
    "        \n",
    "        # Check if all numbers have the same length and >= 5 digits\n",
    "        if len(set(len(str(x)) for x in samples)) == 1:\n",
    "            length = len(str(samples[0]))\n",
    "            if length >= 5:\n",
    "                # Check if all numbers start with the same digit\n",
    "                first_digits = str(samples[0])[0]\n",
    "                return all(str(x).startswith(first_digit) for x in samples)\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def check_year_values(col_data):\n",
    "    if pd.api.types.is_string_dtype(col_data):\n",
    "        try:\n",
    "            col_data = pd.to_numeric(col_data)\n",
    "        except:\n",
    "            pass\n",
    "    # Handle if column is numeric and looks like a year\n",
    "    if pd.api.types.is_numeric_dtype(col_data) or  pd.api.types.is_float_dtype(col_data) or  pd.api.types.is_integer_dtype(col_data):\n",
    "        if col_data.dropna().empty == False:\n",
    "            if pd.api.types.is_float_dtype(col_data):\n",
    "            # Check if float values have only 2 decimal places\n",
    "                if col_data.dropna().apply(lambda x: round(x, 2) == x).all():\n",
    "                    if col_data.dropna().between(1800, 2050).all():\n",
    "                        return True # Only year\n",
    "            # For integer values\n",
    "            elif col_data.dropna().between(1800, 2100).all():\n",
    "                True # Only year\n",
    "                \n",
    "    return False\n",
    "\n",
    "def check_date_format(col_str):\n",
    "    # Check for common date formats (yyyy-mm-dd, dd-mm-yyyy, etc.)\n",
    "    date_pattern = r'^\\d{1,4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,4}$'\n",
    "    return col_str.str.match(date_pattern).all()\n",
    "\n",
    "def check_datetime_format(col_str):\n",
    "    # Check for datetime format (date + time)\n",
    "    datetime_pattern = r'\\d{1,4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,4}.*\\d{1,2}:\\d{2}'\n",
    "    return col_str.str.contains(datetime_pattern).all()\n",
    "\n",
    "def detectIdTypeCol(col_data):\n",
    "    if pd.api.types.is_string_dtype(col_data):\n",
    "        pattern = r'\\b[A-Z0-9]{1,4}[-_./]?[A-Z0-9]{2,6}[-_./]?[A-Z0-9]{0,5}\\b'\n",
    "        return all(re.fullmatch(pattern, item) for item in col_data)\n",
    "    \n",
    "def OneOr2digitDetection(col_data):\n",
    "    non2Digit=0\n",
    "    for i,v  in enumerate(col_data):\n",
    "        if len(str(v)) > 2 and str(v)!='nan':\n",
    "            non2Digit=non2Digit+1\n",
    "    if len(col_data)/2.2 > non2Digit:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def detectSimpleDtypes(col_data):\n",
    "    if pd.api.types.is_integer_dtype(col_data):\n",
    "        return 'allInt'\n",
    "    if pd.api.types.is_float_dtype(col_data):\n",
    "        if col_data.isna().all()== False:\n",
    "            return 'numaric'\n",
    "    if pd.api.types.is_string_dtype(col_data) or pd.api.types.is_object_dtype(col_data):\n",
    "        return 'str'\n",
    "    return None\n",
    "\n",
    "def clusterDateTimeCol(fContent, col,no,ascending=True):\n",
    "    if no ==1:\n",
    "        # Try to detect and sort if the column is just year values\n",
    "        fContent = fContent.sort_values(by=col,ignore_index=True, ascending=ascending)\n",
    "        fContent = fContent.reset_index(drop=True)\n",
    "    elif no == 2 or 3:\n",
    "        # Now, try to detect proper date/datetime columns\n",
    "        try:\n",
    "            # Avoid processing numeric-only or zero-filled columns\n",
    "            sample_vals = fContent[col].astype(str).str.strip().replace('0', np.nan).dropna()\n",
    "            if len(sample_vals) == 0:\n",
    "                return fContent# All values are zero or empty-like\n",
    "            \n",
    "            # Try parsing\n",
    "            try:\n",
    "                parsed_col = pd.to_datetime(fContent[col], errors='raise')\n",
    "                print('try')\n",
    "            except Exception as e:\n",
    "                parsed_col = pd.to_datetime(fContent[col], errors='raise',dayfirst=True)\n",
    "                print(e)\n",
    "               \n",
    "            if all(parsed_col.dt.time == pd.to_datetime('00:00:00').time()) and no == 2:  # Only date\n",
    "                fContent[col] = parsed_col\n",
    "                fContent = clean_and_sort_date_column(fContent, col, ascending)\n",
    "                fContent = fContent.reset_index(drop=True)\n",
    "                # Replace original column with parsed datetime values\n",
    "            elif no == 3:  # Date + time\n",
    "                fContent[col] = parsed_col\n",
    "                fContent = handle_datetime_column(fContent, col, ascending)\n",
    "                fContent = fContent.reset_index(drop=True)\n",
    "        except Exception as e:\n",
    "            return fContent # Not a datetime column\n",
    "        \n",
    "    return fContent \n",
    "\n",
    "def clean_and_sort_date_column(dff, column_name, ascending=True):\n",
    "        try:\n",
    "            \n",
    "            # Step 2: Drop NaT (invalid formats)\n",
    "            dff = dff.dropna(subset=[column_name])\n",
    "            \n",
    "            # Step 3: Sort the DataFrame by that column\n",
    "            dff = dff.sort_values(by=column_name, ascending=ascending)\n",
    "\n",
    "            # Optional: Format to clean date string (YYYY-MM-DD)\n",
    "            dff[column_name] = dff[column_name].dt.strftime('%Y-%m-%d')\n",
    "            print(f\"{column_name} date called \")\n",
    "            return dff\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error while processing date column: {e}\")\n",
    "            return dff\n",
    "\n",
    "def handle_datetime_column(df, column_name, ascending):\n",
    "    print(f\"{column_name} dateTime\")\n",
    "    # Check if most values in column are datetime with time\n",
    "    values = df[column_name].dropna().astype(str).head(20)\n",
    "    count_datetime = sum([pd.api.is_datetime(v) for v in values])\n",
    "\n",
    "    if count_datetime >= len(values) // 2:  # At least half must be datetime-like\n",
    "        # Convert full column to datetime\n",
    "        df[column_name] = pd.to_datetime(df[column_name], errors='coerce')\n",
    "        # Drop rows with invalid dates\n",
    "        df = df.dropna(subset=[column_name])\n",
    "        # Sort by that column\n",
    "        df = df.sort_values(by=column_name,ascending=ascending).reset_index(drop=True)\n",
    "        print(f\"[INFO] '{column_name}' successfully recognized and sorted as datetime.\")\n",
    "    else:\n",
    "        print(f\"[INFO] '{column_name}' does not contain proper datetime with time.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26207e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measurCir(qc,q_num):\n",
    "    qc.measure(q_num,q_num)\n",
    "    simulator = AerSimulator()\n",
    "    # Transpile & run\n",
    "    compiled = transpile(qc, simulator)\n",
    "    r = simulator.run(compiled, shots=100000).result()\n",
    "    counts = r.get_counts()\n",
    "    for k,v in counts.items():\n",
    "        counts[k] = int(v/1000)\n",
    "    return counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
