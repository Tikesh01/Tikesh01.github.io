{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5b00335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'Suppressed': {'0': 89, '1': 10}}\n"
     ]
    }
   ],
   "source": [
    "#Sort Different columns: \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit_aer.primitives import Sampler\n",
    "import re\n",
    "import openpyxl\n",
    "\n",
    "df = pd.read_csv('Files/business-financial-data-december-2024-quarter-csv.csv')\n",
    "type = detectColumns(df,['Suppressed'])\n",
    "print(type)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def digitSorting(df,col,asc):\n",
    "    df[col] = df[col].astype(str)\n",
    "    df = df.sort_values(by=col,ascending=asc, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv(\"Files/student_dataset.csv\")\n",
    "df = digitSorting(df,'Grade',asc=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiIndex(dataFrame, colToCheck, colType, yearOnly = False, asc=True):\n",
    "    # Step 1: Get last indices of each year\n",
    "    if yearOnly == True:\n",
    "        yearsWithLastIndex = get_last_indices_of_each_year(dataFrame[colToCheck], True)\n",
    "    else: \n",
    "        if (colType['0'] == 30 or  colType['1'] == 70) or (colType['0'] == 20 or  colType['1'] == 80):\n",
    "            yearsWithLastIndex = get_last_indices_of_each_year(pd.to_datetime(dataFrame[colToCheck]),False)\n",
    "        if(colType['1']==30 or colType['0']==70) or (colType['1'] ==50 or colType['0']==50):\n",
    "            yearsWithLastIndex = get_last_indices_of_each_year(dataFrame[colToCheck],False,True,asc=asc)\n",
    "            asc=True\n",
    "        else:\n",
    "            yearsWithLastIndex = get_last_indices_of_each_year(dataFrame[colToCheck], False)\n",
    "    if asc==False:\n",
    "        yearsWithLastIndex = dict(reversed(list(yearsWithLastIndex.items())))\n",
    "        \n",
    "    nameOfGroups = list(yearsWithLastIndex.keys())\n",
    "    last_indices = list(yearsWithLastIndex.values())\n",
    "\n",
    "    # Step 2: Compute counts from last indices\n",
    "    group_sizes = []\n",
    "    prev = -1\n",
    "    for idx in last_indices:\n",
    "        group_sizes.append(idx - prev)\n",
    "        prev = idx\n",
    "    print(group_sizes)\n",
    "    # Step 3: Create array per group\n",
    "    objOfGroups = {\n",
    "        f'key{i}': np.array([f'Group of {year}'] * group_sizes[i]) for i, year in enumerate(nameOfGroups)\n",
    "    }\n",
    "    \n",
    "    # Step 4: Combine into one array\n",
    "    outside = np.concatenate(list(objOfGroups.values()))\n",
    "    inside = np.arange(len(outside))\n",
    "        \n",
    "    print(outside)\n",
    "    # Step 5: Create inside index\n",
    "    \n",
    "    multi_index = pd.MultiIndex.from_arrays([outside, inside], names=[\"Group\", \"Sr No.\"])\n",
    "    \n",
    "    dataFrame = dataFrame.reindex(range(len(multi_index)))\n",
    "    dataFrame.set_index(multi_index,inplace=True,)\n",
    "    print(\"Multicalled\")\n",
    "    return dataFrame\n",
    "\n",
    "def get_last_indices_of_each_year(date_series, YearOnly=False,rol=False, a=0.60,asc=True):\n",
    "    \n",
    "    # data_series = data_series.apply(pd.to_numeric, errors='coerce').astype('Int64')\n",
    "    if rol==True and findDuplicate(date_series).count() <=8:\n",
    "            df = pd.DataFrame({'year':date_series},index=np.arange(len(date_series))).astype(str)\n",
    "            df['half'] = df['year'].apply(lambda x: x[:round(len(str(x))*a)])\n",
    "            print(a)\n",
    "            last_indices =  df.groupby('half',).apply(lambda x: x.index[-1]).to_dict()\n",
    "            if asc ==False:\n",
    "                last_indices = dict(reversed(list(last_indices.items())))\n",
    "                \n",
    "            group_sizes = []\n",
    "            prev = -1\n",
    "            k=0\n",
    "            for i,idx in enumerate(last_indices.values()):\n",
    "                group_sizes.append(idx - prev)\n",
    "                prev = idx\n",
    "                if group_sizes[i]<= 15:\n",
    "                    k=k+1\n",
    "            if int(len(last_indices)*0.40)<=k and a>=0.10:\n",
    "                last_indices = get_last_indices_of_each_year(date_series,False,True,a=a-0.10, asc=asc)\n",
    "            return last_indices\n",
    "    if YearOnly == True:\n",
    "        print(\"yearOnly work\")\n",
    "        df = pd.DataFrame({'year': date_series}, index=np.arange(len(date_series)))\n",
    "    \n",
    "    else:\n",
    "        # Extract year\n",
    "        try:\n",
    "            years = date_series.dt.year\n",
    "            # Create a DataFrame with index\n",
    "            df = pd.DataFrame({'year': years}, index=np.arange(len(date_series)))\n",
    "            print(\"Year Extracted\")\n",
    "        except:\n",
    "            #Create a DataFrame with index\n",
    "            df = pd.DataFrame({'year': date_series}, index=np.arange(len(date_series)))\n",
    "        \n",
    "    # Get last index of each year group\n",
    "    last_indices = df.groupby('year').apply(lambda x: x.index[-1]).to_dict()\n",
    "        \n",
    "    print(\"last index called\")\n",
    "    return last_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def sortRollCol(df, col,asc):\n",
    "    digits_df = df[col].astype(str).apply(lambda x: pd.Series(list(x)))\n",
    "    \n",
    "    digits_df['original_index'] = df.index\n",
    "    \n",
    "    sorted_digits_df = recursiveSort(digits_df)\n",
    "\n",
    "    sorted_indices = sorted_digits_df['original_index'].values\n",
    "    if asc == False:\n",
    "        sorted_indices = reversed(sorted_indices)\n",
    "        \n",
    "    sorted_df = df.loc[sorted_indices].reset_index(drop=True)\n",
    "\n",
    "    return sorted_df\n",
    "\n",
    "\n",
    "def recursiveSort(df_digits, col=0):\n",
    "    if col >= int(len(df_digits.columns) - 1):  # exclude 'original_index' column\n",
    "        return df_digits\n",
    "\n",
    "    # Sort by the current digit column\n",
    "    df_digits = df_digits.sort_values(by=col, kind='stable', ignore_index=True)\n",
    "\n",
    "    # Group by current digit and recursively sort each group\n",
    "    result = []\n",
    "    for value, group in df_digits.groupby(col, sort=False):\n",
    "        sorted_group = recursiveSort(group.reset_index(drop=True), col + 1)\n",
    "        result.append(sorted_group)\n",
    "    \n",
    "    return pd.concat(result, ignore_index=True)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "df = pd.read_csv('Files/student_dataset.csv')  # Your dataset with 'Roll No.'\n",
    "asc = False\n",
    "df = sortRollCol(df,'Roll No.',asc=asc)\n",
    "type = {\n",
    "        '0' : 70, \n",
    "        '1' : 30\n",
    "        }\n",
    "df = multiIndex(df,colToCheck='Roll No.',colType=type,asc=asc)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06935ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataClean(df,colTypes):\n",
    "    for i in df.index[(df.isna().sum(axis=1)==len(df.columns))]:\n",
    "        df.drop(index=i, inplace=True)\n",
    "        \n",
    "    for i in colTypes.keys():\n",
    "        if df[i].isna().sum() >=len(df[i])-2:\n",
    "            df.drop(columns=[i],inplace=True)\n",
    "            continue\n",
    "        \n",
    "        if df[i].isna().sum() != 0:#IF column has no type\n",
    "                print(i)\n",
    "                if (colTypes[i]['0'] == 60 or colTypes[i]['1'] == 40): # if column is type of year only\n",
    "                    df[i] = df[i].fillna(2100)\n",
    "                elif colTypes[i]['0'] == 30 or  colTypes[i]['1'] == 70:#if column is type of date only \n",
    "                    df[i] = df[i].fillna(pd.to_datetime('2030-01-01'))\n",
    "                elif colTypes[i]['0'] == 20 or  colTypes[i]['1'] == 80 :#if column is type of date and time \n",
    "                    df[i]= df[i].fillna('2030-01-01')\n",
    "                elif colTypes[i]['0'] == 70 or colTypes[i]['1']==30: #Roll no type\n",
    "                    pass\n",
    "                elif colTypes[i]['1']==50 or colTypes[i]['0']==50:#if it is of type id \n",
    "                    df[i]=df[i].fillna(\"Unknown\")\n",
    "                elif colTypes[i]['0']==90 or colTypes[i]['1'] ==10:#if it of type oneor2digit\n",
    "                    df[i] = df[i].fillna(0)\n",
    "                elif colTypes[i]['0'] == 80 or  colTypes[i]['1'] == 20 : #if it type of nemric\n",
    "                    df[i]=df[i].fillna(0.0)\n",
    "                elif colTypes[i]['0'] == 40 or  colTypes[i]['1'] == 60 : #string or object\n",
    "                    df[i] = df[i].fillna('NAN')\n",
    "        continue\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv('Files/business-financial-data-december-2024-quarter-csv.csv')\n",
    "col = detectColumns(df,df.columns)\n",
    "print(df.isna().sum())\n",
    "df = dataClean(df,col)\n",
    "print(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd8dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDuplicate(series):\n",
    "    series = series[series.duplicated(keep=False)]\n",
    "    return series\n",
    "\n",
    "df = pd.read_csv('Files/twitter_training.csv')\n",
    "dup = findDuplicate(df['Comments'])\n",
    "print(dup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34b6ced4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time By Model  44.38958549499512\n",
      "Time in Normalizig : 0.003229379653930664\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m_openmp_helpers.pyx:66\u001b[39m, in \u001b[36msklearn.utils._openmp_helpers._openmp_effective_n_threads\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: True",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m df = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mFiles/employee_time_log.csv\u001b[39m\u001b[33m'\u001b[39m) \n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mkmeansOnString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mEmployee_ID\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mkmeansOnString\u001b[39m\u001b[34m(df, col)\u001b[39m\n\u001b[32m     26\u001b[39m k =  \u001b[38;5;28mint\u001b[39m(np.ceil(\u001b[38;5;28mlen\u001b[39m(df[col]) / \u001b[32m80\u001b[39m))\u001b[38;5;66;03m# number of clusters\u001b[39;00m\n\u001b[32m     27\u001b[39m kmeans = KMeans(n_clusters=\u001b[32m7\u001b[39m, n_init=\u001b[32m19\u001b[39m, max_iter=\u001b[32m450\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mCluster Id\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mkmeans\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m et = time.time()\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTime clustering : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00met-st\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1064\u001b[39m, in \u001b[36m_BaseKMeans.fit_predict\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1042\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001b[39;00m\n\u001b[32m   1043\u001b[39m \n\u001b[32m   1044\u001b[39m \u001b[33;03m    Convenience method; equivalent to calling fit(X) followed by\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1062\u001b[39m \u001b[33;03m        Index of the cluster each sample belongs to.\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m.labels_\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1468\u001b[39m, in \u001b[36mKMeans.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1466\u001b[39m random_state = check_random_state(\u001b[38;5;28mself\u001b[39m.random_state)\n\u001b[32m   1467\u001b[39m sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\u001b[32m-> \u001b[39m\u001b[32m1468\u001b[39m \u001b[38;5;28mself\u001b[39m._n_threads = \u001b[43m_openmp_effective_n_threads\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[38;5;66;03m# Validate init array\u001b[39;00m\n\u001b[32m   1471\u001b[39m init = \u001b[38;5;28mself\u001b[39m.init\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_openmp_helpers.pyx:21\u001b[39m, in \u001b[36msklearn.utils._openmp_helpers._openmp_effective_n_threads\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_openmp_helpers.pyx:68\u001b[39m, in \u001b[36msklearn.utils._openmp_helpers._openmp_effective_n_threads\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:638\u001b[39m, in \u001b[36mcpu_count\u001b[39m\u001b[34m(only_physical_cores)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    636\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloky\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43monly_physical_cores\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_physical_cores\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:129\u001b[39m, in \u001b[36mcpu_count\u001b[39m\u001b[34m(only_physical_cores)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cpu_count_user < os_cpu_count:\n\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# Respect user setting\u001b[39;00m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(cpu_count_user, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m cpu_count_physical, exception = \u001b[43m_count_physical_cores\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cpu_count_physical != \u001b[33m\"\u001b[39m\u001b[33mnot found\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cpu_count_physical\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:257\u001b[39m, in \u001b[36m_count_physical_cores\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    255\u001b[39m     cpu_count_physical = \u001b[38;5;28mlen\u001b[39m(cpu_info)\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m sys.platform == \u001b[33m\"\u001b[39m\u001b[33mwin32\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     cpu_info = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwmic CPU Get NumberOfCores /Format:csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m     cpu_info = cpu_info.stdout.splitlines()\n\u001b[32m    263\u001b[39m     cpu_info = [\n\u001b[32m    264\u001b[39m         l.split(\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m1\u001b[39m]\n\u001b[32m    265\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m cpu_info\n\u001b[32m    266\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (l \u001b[38;5;129;01mand\u001b[39;00m l != \u001b[33m\"\u001b[39m\u001b[33mNode,NumberOfCores\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    267\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\subprocess.py:552\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m         stdout, stderr = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    554\u001b[39m         process.kill()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\subprocess.py:1211\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1208\u001b[39m     endtime = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1211\u001b[39m     stdout, stderr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1213\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1214\u001b[39m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[32m   1215\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\subprocess.py:1630\u001b[39m, in \u001b[36mPopen._communicate\u001b[39m\u001b[34m(self, input, endtime, orig_timeout)\u001b[39m\n\u001b[32m   1626\u001b[39m \u001b[38;5;66;03m# Wait for the reader threads, or time out.  If we time out, the\u001b[39;00m\n\u001b[32m   1627\u001b[39m \u001b[38;5;66;03m# threads remain reading and the fds left open in case the user\u001b[39;00m\n\u001b[32m   1628\u001b[39m \u001b[38;5;66;03m# calls communicate again.\u001b[39;00m\n\u001b[32m   1629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stdout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1630\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstdout_thread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_remaining_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stdout_thread.is_alive():\n\u001b[32m   1632\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m TimeoutExpired(\u001b[38;5;28mself\u001b[39m.args, orig_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\threading.py:1149\u001b[39m, in \u001b[36mThread.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1146\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot join current thread\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1149\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1151\u001b[39m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[32m   1152\u001b[39m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[32m   1153\u001b[39m     \u001b[38;5;28mself\u001b[39m._wait_for_tstate_lock(timeout=\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tikes\\miniconda3\\Lib\\threading.py:1169\u001b[39m, in \u001b[36mThread._wait_for_tstate_lock\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1169\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1170\u001b[39m         lock.release()\n\u001b[32m   1171\u001b[39m         \u001b[38;5;28mself\u001b[39m._stop()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Files/employee_time_log.csv') \n",
    "df = kmeansOnString(df,'Employee_ID')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cde259db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tikes\\AppData\\Local\\Temp\\ipykernel_15320\\3824584049.py:4: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[0][i] = df[0][i][:df[0][i].find('.')]+df[0][i][df[0][i].find('.')+1:]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bdca930-y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bdcauei-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ueii/783-7ueii/783-77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0\n",
       "0              bdca930-y\n",
       "1              bdcauei-9\n",
       "2  ueii/783-7ueii/783-77"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ['bdca.930-y','bdca.uei-9','ueii/783-77']\n",
    "df = pd.DataFrame(x)\n",
    "for i,v in enumerate(df[0]):\n",
    "    df[0][i] = df[0][i][:df[0][i].find('.')]+df[0][i][df[0][i].find('.')+1:]\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "952da5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "def kmeansOnString(df,col):\n",
    "    phrase = (df[col]).astype(str)\n",
    "    # Step 1: Convert strings to embeddings using a Sentence Transformer (BERT-based model)\n",
    "    st = time.time()\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')  # a lightweight, efficient model\n",
    "    embeddings = model.encode(phrase)\n",
    "    et = time.time()\n",
    "    print(\"time By Model \", et-st)\n",
    "    \n",
    "    st = et\n",
    "    # Optionally, normalize embeddings if needed:\n",
    "    from sklearn.preprocessing import normalize\n",
    "    embeddings = normalize(embeddings)\n",
    "    et = time.time()\n",
    "    print(f\"Time in Normalizig : {et-st}\")\n",
    "    \n",
    "    st = et\n",
    "    # Step 2: Apply KMeans clustering on these embeddings\n",
    "    # Using n_init and max_iter helps ensure stable convergence.\n",
    "    k =  int(np.ceil(len(df[col]) / 80))# number of clusters\n",
    "    kmeans = KMeans(n_clusters=7, n_init=19, max_iter=450, random_state=42)\n",
    "    df['Cluster Id'] = kmeans.fit_predict(embeddings)\n",
    "    et = time.time()\n",
    "    print(f\"Time clustering : {et-st}\")\n",
    "    \n",
    "    st = et\n",
    "    # Step 3: Evaluate the clustering with Silhouette Score\n",
    "    score = silhouette_score(embeddings, df['Cluster Id'])\n",
    "    print(\"Silhouette Score:\", score)\n",
    "    et = time.time()\n",
    "    print(f\"Time in Normalizig : {et-st}\")\n",
    "    \n",
    "    df = df.sort_values(by='Cluster Id').drop(columns=['Cluster Id'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91fbbf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit\n",
    "import pandas as pd\n",
    "import math\n",
    "def detectColumns(df, prioColumns):\n",
    "    result = {}    \n",
    "    for col in prioColumns:\n",
    "        qc =  QuantumCircuit(1,1)\n",
    "        col_data = df[col]\n",
    "        col_str = df[col].astype(str).str.strip()\n",
    "        # 3. Check for Date values (type 2)\n",
    "        if check_date_format(col_str):\n",
    "            p = 0.70000\n",
    "                \n",
    "        # 4. Check for DateTime values (type 3)\n",
    "        elif check_datetime_format(col_str):\n",
    "            p = 0.800000\n",
    "            \n",
    "        elif OneOr2digitDetection(col_data):\n",
    "            p = 0.100000\n",
    "        # 1. Check for Roll Numbers (type 4)\n",
    "        elif pd.api.types.is_numeric_dtype(col_data):\n",
    "            p = 0.2000\n",
    "            # 2. Check for Year values (type 1)\n",
    "            if check_year_values(col_data):\n",
    "                p = 0.40000\n",
    "            elif check_roll_number(col_data):\n",
    "                p = 0.30000\n",
    "\n",
    "        elif pd.api.types.is_string_dtype(df[col]) or pd.api.types.is_object_dtype(df[col]):\n",
    "            p = 0.600000\n",
    "            if detectIdTypeCol(col_data):\n",
    "                p = 0.5\n",
    "        \n",
    "        angle = 2 * math.asin(math.sqrt(p))\n",
    "        qc.ry(angle, 0)\n",
    "        # Initialize result as 0 (unrecognized type)\n",
    "        result[col] = measurCir(qc,0)\n",
    "        \n",
    "    return result\n",
    "\n",
    "def check_roll_number(col_data):\n",
    "    try:\n",
    "        # Convert numbers to strings for checking patterns\n",
    "        sample_start = [str(i) for i in col_data.head(10)]\n",
    "        sample_middle = [str(i) for i in col_data.iloc[int(len(col_data)/2)-5:int(len(col_data)/2)+5]]\n",
    "        sample_end = [str(i) for i in col_data.iloc[-10:]]\n",
    "        \n",
    "        # Combine samples\n",
    "        samples = sample_start + sample_middle + sample_end\n",
    "        \n",
    "        # Check if all numbers have the same length and >= 5 digits\n",
    "        if len(set(len(str(x)) for x in samples)) == 1:\n",
    "            length = len(str(samples[0]))\n",
    "            if length >= 5:\n",
    "                # Check if all numbers start with the same digit\n",
    "                first_digits = str(samples[0])[0]\n",
    "                return all(str(x).startswith(first_digit) for x in samples)\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def check_year_values(col_data):\n",
    "    if pd.api.types.is_string_dtype(col_data):\n",
    "        try:\n",
    "            col_data = pd.to_numeric(col_data)\n",
    "        except:\n",
    "            pass\n",
    "    # Handle if column is numeric and looks like a year\n",
    "    if pd.api.types.is_numeric_dtype(col_data) or  pd.api.types.is_float_dtype(col_data) or  pd.api.types.is_integer_dtype(col_data):\n",
    "        if col_data.dropna().empty == False:\n",
    "            if pd.api.types.is_float_dtype(col_data):\n",
    "            # Check if float values have only 2 decimal places\n",
    "                if col_data.dropna().apply(lambda x: round(x, 2) == x).all():\n",
    "                    if col_data.dropna().between(1800, 2050).all():\n",
    "                        return True # Only year\n",
    "            # For integer values\n",
    "            elif col_data.dropna().between(1800, 2100).all():\n",
    "                True # Only year\n",
    "                \n",
    "    return False\n",
    "\n",
    "def check_date_format(col_str):\n",
    "    # Check for common date formats (yyyy-mm-dd, dd-mm-yyyy, etc.)\n",
    "    date_pattern = r'^\\d{1,4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,4}$'\n",
    "    return col_str.str.match(date_pattern).all()\n",
    "\n",
    "def check_datetime_format(col_str):\n",
    "    # Check for datetime format (date + time)\n",
    "    datetime_pattern = r'\\d{1,4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,4}.*\\d{1,2}:\\d{2}'\n",
    "    return col_str.str.contains(datetime_pattern).all()\n",
    "\n",
    "def detectIdTypeCol(col_data):\n",
    "    if pd.api.types.is_string_dtype(col_data):\n",
    "        pattern = r'\\b[A-Z0-9]{1,4}[-_./]?[A-Z0-9]{2,6}[-_./]?[A-Z0-9]{0,5}\\b'\n",
    "        return all(re.fullmatch(pattern, item) for item in col_data)\n",
    "    \n",
    "def OneOr2digitDetection(col_data):\n",
    "    non2Digit=0\n",
    "    for i,v  in enumerate(col_data):\n",
    "        if len(str(v)) > 2 and str(v)!='nan':\n",
    "            non2Digit=non2Digit+1\n",
    "    print(non2Digit)\n",
    "    if len(col_data)/2.2 > non2Digit:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def detectSimpleDtypes(col_data):\n",
    "    if pd.api.types.is_integer_dtype(col_data):\n",
    "        return 'allInt'\n",
    "    if pd.api.types.is_float_dtype(col_data):\n",
    "        if col_data.isna().all()== False:\n",
    "            return 'numaric'\n",
    "    if pd.api.types.is_string_dtype(col_data) or pd.api.types.is_object_dtype(col_data):\n",
    "        return 'str'\n",
    "    return None\n",
    "\n",
    "def clusterDateTimeCol(fContent, col,no,ascending=True):\n",
    "    if no ==1:\n",
    "        # Try to detect and sort if the column is just year values\n",
    "        fContent = fContent.sort_values(by=col,ignore_index=True, ascending=ascending)\n",
    "        fContent = fContent.reset_index(drop=True)\n",
    "    elif no == 2 or 3:\n",
    "        # Now, try to detect proper date/datetime columns\n",
    "        try:\n",
    "            # Avoid processing numeric-only or zero-filled columns\n",
    "            sample_vals = fContent[col].astype(str).str.strip().replace('0', np.nan).dropna()\n",
    "            if len(sample_vals) == 0:\n",
    "                return fContent# All values are zero or empty-like\n",
    "            \n",
    "            # Try parsing\n",
    "            try:\n",
    "                parsed_col = pd.to_datetime(fContent[col], errors='raise')\n",
    "            except:\n",
    "                parsed_col = pd.to_datetime(fContent[col], dayfirst=True, errors='raise')\n",
    "               \n",
    "            if all(parsed_col.dt.time == pd.to_datetime('00:00:00').time()) and no == 2:  # Only date\n",
    "                fContent[col] = parsed_col\n",
    "                fContent = clean_and_sort_date_column(fContent, col, ascending)\n",
    "                fContent = fContent.reset_index(drop=True)\n",
    "                # Replace original column with parsed datetime values\n",
    "            elif no == 3:  # Date + time\n",
    "                fContent[col] = parsed_col\n",
    "                fContent = handle_datetime_column(fContent, col, ascending)\n",
    "                fContent = fContent.reset_index(drop=True)\n",
    "        except Exception as e:\n",
    "            return fContent # Not a datetime column\n",
    "        \n",
    "    return fContent \n",
    "\n",
    "def clean_and_sort_date_column(dff, column_name, ascending=True):\n",
    "        try:\n",
    "            \n",
    "            # Step 2: Drop NaT (invalid formats)\n",
    "            dff = dff.dropna(subset=[column_name])\n",
    "            \n",
    "            # Step 3: Sort the DataFrame by that column\n",
    "            dff = dff.sort_values(by=column_name, ascending=ascending)\n",
    "\n",
    "            # Optional: Format to clean date string (YYYY-MM-DD)\n",
    "            dff[column_name] = dff[column_name].dt.strftime('%Y-%m-%d')\n",
    "            print(f\"{column_name} date called \")\n",
    "            return dff\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error while processing date column: {e}\")\n",
    "            return dff\n",
    "\n",
    "def handle_datetime_column(df, column_name, ascending):\n",
    "    print(f\"{column_name} dateTime\")\n",
    "    # Check if most values in column are datetime with time\n",
    "    values = df[column_name].dropna().astype(str).head(20)\n",
    "    count_datetime = sum([pd.api.is_datetime(v) for v in values])\n",
    "\n",
    "    if count_datetime >= len(values) // 2:  # At least half must be datetime-like\n",
    "        # Convert full column to datetime\n",
    "        df[column_name] = pd.to_datetime(df[column_name], errors='coerce')\n",
    "        # Drop rows with invalid dates\n",
    "        df = df.dropna(subset=[column_name])\n",
    "        # Sort by that column\n",
    "        df = df.sort_values(by=column_name,ascending=ascending).reset_index(drop=True)\n",
    "        print(f\"[INFO] '{column_name}' successfully recognized and sorted as datetime.\")\n",
    "    else:\n",
    "        print(f\"[INFO] '{column_name}' does not contain proper datetime with time.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26207e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measurCir(qc,q_num):\n",
    "    qc.measure(q_num,q_num)\n",
    "    simulator = AerSimulator()\n",
    "    # Transpile & run\n",
    "    compiled = transpile(qc, simulator)\n",
    "    r = simulator.run(compiled, shots=100000).result()\n",
    "    counts = r.get_counts()\n",
    "    for k,v in counts.items():\n",
    "        counts[k] = int(v/1000)\n",
    "    return counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
