{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a5b00335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': {'0': 89, '1': 10}, 'name': {'1': 60, '0': 39}, 'properties': {'0': 39, '1': 60}}\n"
     ]
    }
   ],
   "source": [
    "#Sort Different columns: \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit_aer.primitives import Sampler\n",
    "import re\n",
    "import openpyxl\n",
    "\n",
    "df = pd.read_excel('Files/Student-1.xlsx')\n",
    "type = detectColumns(df,df.columns)\n",
    "print(type)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a003e305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Student_Names   Phone_No.  Math  Physics  Chemistry Grade  \\\n",
      "0      Karina Williams  9092378235    23       26         36     F   \n",
      "1       Kristin Parker  9226776504    12       25         38     F   \n",
      "2           Amber Leon  9932583197    38       23         17     F   \n",
      "3         Jodi Kennedy  9443258069    17       26         45     F   \n",
      "4     Natasha Stephens  9828839720    29       32         11     F   \n",
      "...                ...         ...   ...      ...        ...   ...   \n",
      "8995     Edward Palmer  9817654825    98       81         84     A   \n",
      "8996      Tracy Miller  9176542338    76      100         89     A   \n",
      "8997      Nichole Snow  9108855306    93       62         98     A   \n",
      "8998      Peter Maddox  9967091167    71       89         90     A   \n",
      "8999     Marissa Smith  9601081991    96       51        100     A   \n",
      "\n",
      "                   Comment  Roll No.           School Name  \\\n",
      "0                   Failed    536668  Martin Luther School   \n",
      "1                   Failed    550199  Martin Luther School   \n",
      "2                   Failed    588985  Martin Luther School   \n",
      "3                   Failed    526276  Martin Luther School   \n",
      "4                   Failed    594811  Martin Luther School   \n",
      "...                    ...       ...                   ...   \n",
      "8995  Very Good Achivement    528552  Martin Luther School   \n",
      "8996  Very Good Achivement    500544  Martin Luther School   \n",
      "8997  Very Good Achivement    550932  Martin Luther School   \n",
      "8998  Very Good Achivement    559773  Martin Luther School   \n",
      "8999  Very Good Achivement    544303  Martin Luther School   \n",
      "\n",
      "                                        Student Address  \n",
      "0     4761 Rivera Field Suite 609, North Mauriceton,...  \n",
      "1               1634 Wood Drive, South Dennis, MT 27475  \n",
      "2     31786 Hill Crescent Suite 527, Danielfurt, KY ...  \n",
      "3     6329 White Station Apt. 103, Christopherside, ...  \n",
      "4          722 Morgan Motorway, West Jonathan, DE 08198  \n",
      "...                                                 ...  \n",
      "8995       0524 Salinas Valley, Fernandezfurt, OH 78395  \n",
      "8996  6676 Jimenez Oval Apt. 888, Thomasburgh, KS 87803  \n",
      "8997      8348 Morales Forges, Macdonaldburgh, SC 52458  \n",
      "8998  7785 Nancy Ramp Suite 252, Port Curtisbury, FM...  \n",
      "8999  65736 Victoria Shoal Suite 710, Woodsbury, MI ...  \n",
      "\n",
      "[9000 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def digitSorting(df,col,asc):\n",
    "    df[col] = df[col].astype(str)\n",
    "    df = df.sort_values(by=col,ascending=asc, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv(\"Files/student_dataset.csv\")\n",
    "df = digitSorting(df,'Grade',asc=False)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dad1b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def sortRollCol(df, col,asc):\n",
    "    digits_df = df[col].astype(str).apply(lambda x: pd.Series(list(x)))\n",
    "    \n",
    "    digits_df['original_index'] = df.index\n",
    "    \n",
    "    sorted_digits_df = recursiveSort(digits_df)\n",
    "\n",
    "    sorted_indices = sorted_digits_df['original_index'].values\n",
    "    if asc == False:\n",
    "        sorted_indices = reversed(sorted_indices)\n",
    "        \n",
    "    sorted_df = df.loc[sorted_indices].reset_index(drop=True)\n",
    "\n",
    "    return sorted_df\n",
    "\n",
    "\n",
    "def recursiveSort(df_digits, col=0):\n",
    "    if col >= int(len(df_digits.columns) - 1):  # exclude 'original_index' column\n",
    "        return df_digits\n",
    "\n",
    "    # Sort by the current digit column\n",
    "    df_digits = df_digits.sort_values(by=col, kind='stable', ignore_index=True)\n",
    "\n",
    "    # Group by current digit and recursively sort each group\n",
    "    result = []\n",
    "    for value, group in df_digits.groupby(col, sort=False):\n",
    "        sorted_group = recursiveSort(group.reset_index(drop=True), col + 1)\n",
    "        result.append(sorted_group)\n",
    "    \n",
    "    return pd.concat(result, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06935ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def dataClean(df,colTypes):\n",
    "    for i in df.index[(df.isna().sum(axis=1)==len(df.columns))]:\n",
    "        df.drop(index=i, inplace=True)\n",
    "        \n",
    "    for i in colTypes.keys():\n",
    "        if df[i].isna().sum() >=len(df[i])-2:\n",
    "            df.drop(columns=[i],inplace=True)\n",
    "            continue\n",
    "        \n",
    "        if df[i].isna().sum() != 0:#IF column has no type\n",
    "                print(i)\n",
    "                if (colTypes[i]['0'] == 60 or colTypes[i]['1'] == 40): # if column is type of year only\n",
    "                    df[i] = df[i].fillna(2099)\n",
    "                elif colTypes[i]['0'] == 30 or  colTypes[i]['1'] == 70:#if column is type of date only \n",
    "                    df[i] = df[i].fillna(pd.to_datetime('2030-01-01'))\n",
    "                elif colTypes[i]['0'] == 20 or  colTypes[i]['1'] == 80 :#if column is type of date and time \n",
    "                    df[i]= df[i].fillna('2030-01-01')\n",
    "                elif colTypes[i]['0'] == 70 or colTypes[i]['1']==30: #Roll no type\n",
    "                    pass\n",
    "                elif colTypes[i]['1']==50 or colTypes[i]['0']==50:#if it is of type id \n",
    "                    df[i]=df[i].fillna(\"NAN-123-00\")\n",
    "                elif colTypes[i]['0']==90 or colTypes[i]['1'] ==10:#if it of type oneor2digit\n",
    "                    df[i] = df[i].fillna(0)\n",
    "                elif colTypes[i]['0'] == 80 or  colTypes[i]['1'] == 20 : #if it type of nemric\n",
    "                    df[i]=df[i].fillna(0)\n",
    "                elif colTypes[i]['0'] == 40 or  colTypes[i]['1'] == 60 : #string or object\n",
    "                    df[i] = df[i].fillna('NULL-NAN')\n",
    "                    \n",
    "        elif len(set(df[i]))==1:\n",
    "            for k in range(2):\n",
    "                df[i].at[k]=\"Changed For better Clustering\"\n",
    "        continue\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv('Files/business-financial-data-december-2024-quarter-csv.csv')\n",
    "\n",
    "col = detectColumns(df,df.columns)\n",
    "print(df)\n",
    "df = dataClean(df,col)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd8dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "def findDuplicate(series):\n",
    "    series = series[series.duplicated(keep=False)]\n",
    "    half = set(series)\n",
    "    if len(half)>=len( series)*0.30:\n",
    "        return 9\n",
    "    return 11\n",
    "\n",
    "df = pd.read_csv('Files/employee_time_log.csv')\n",
    "dup = findDuplicate(df['Employee_ID'])\n",
    "print(np.array(dup))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26b16365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=2, Silhouette Score=0.0900\n",
      "K=4, Silhouette Score=0.0630\n",
      "hii\n",
      "K=11, Silhouette Score=0.0812\n",
      "hii\n",
      "K=18, Silhouette Score=0.1076\n",
      "hii\n",
      "K=25, Silhouette Score=0.1375\n",
      "hii\n",
      "K=32, Silhouette Score=0.1758\n",
      "hii\n",
      "K=39, Silhouette Score=0.2147\n",
      "bye\n",
      "K=48, Silhouette Score=0.2659\n",
      "bye\n",
      "K=57, Silhouette Score=0.2952\n",
      "bye\n",
      "K=66, Silhouette Score=0.3345\n",
      "kye\n",
      "K=77, Silhouette Score=0.3901\n",
      "kye\n",
      "K=88, Silhouette Score=0.4455\n",
      "kye\n",
      "K=99, Silhouette Score=0.4965\n",
      "{2: np.float32(0.09003331), 4: np.float32(0.06297199), 11: np.float32(0.08122044), 18: np.float32(0.10762339), 25: np.float32(0.13751623), 32: np.float32(0.17578171), 39: np.float32(0.21473537), 48: np.float32(0.26586828), 57: np.float32(0.29524392), 66: np.float32(0.33447242), 77: np.float32(0.39007452), 88: np.float32(0.44553006), 99: np.float32(0.4964698)}\n",
      "{99: np.float32(0.4964698), 88: np.float32(0.44553006)}\n",
      "K=98, Silhouette Score=0.4928\n",
      "K=99, Silhouette Score=0.4965\n",
      "K=100, Silhouette Score=0.5020\n",
      "K=87, Silhouette Score=0.4393\n",
      "K=88, Silhouette Score=0.4455\n",
      "K=89, Silhouette Score=0.4499\n",
      "\n",
      "Best K: 100, Highest Silhouette Score: 0.5020\n",
      "     Index      Customer Id First Name Last Name  \\\n",
      "0      502  8bDC7FCd0bDD1A3     Rhonda      Cook   \n",
      "1      507  f5319CE131F7b18    Michael   Griffin   \n",
      "2      422  c6CAAD726614682      Terry  Woodward   \n",
      "3      331  31f87aAa0c99Ef1     Tracey     Hardy   \n",
      "4      406  70dAab994Ccc3fd    Bethany      Ruiz   \n",
      "..     ...              ...        ...       ...   \n",
      "995    946  3F268B8b63a7Ee1     Monica     Joyce   \n",
      "996    142  1FbEcaef8fACcCA     Autumn    Cuevas   \n",
      "997    529  DBa86561f0eb9EE    Richard   Vasquez   \n",
      "998    956  27cAAd4e0DD5598      Janet   Stevens   \n",
      "999     11  bf104C25d0BA4E1    Nichole    Cannon   \n",
      "\n",
      "                           Company                 City  \\\n",
      "0                   Pennington-Lee          Zamoraburgh   \n",
      "1        Finley, Molina and Ortega      West Sophiafort   \n",
      "2                        Hicks Inc          Schultzbury   \n",
      "3                Valentine-Murillo        Blanchardfurt   \n",
      "4    Chambers, Castillo and Graves  South Andreachester   \n",
      "..                             ...                  ...   \n",
      "995        Pham, Murphy and Watson          Howardville   \n",
      "996                       Hahn Ltd            Mccoyfort   \n",
      "997                      Glenn PLC         Stanleyhaven   \n",
      "998     Delgado, Nixon and Nielsen         Perkinshaven   \n",
      "999                  Rios and Sons           West Devon   \n",
      "\n",
      "                    Country                 Phone 1                 Phone 2  \\\n",
      "0                   Lebanon                   -6373     +1-245-439-4377x124   \n",
      "1      Syrian Arab Republic     (520)429-5635x25615        001-540-530-6719   \n",
      "2      Syrian Arab Republic           (025)888-2467       395-954-3146x8730   \n",
      "3    Libyan Arab Jamahiriya       503.865.4828x2502           (841)107-6231   \n",
      "4      Syrian Arab Republic  001-008-242-5333x78184           (589)797-6403   \n",
      "..                      ...                     ...                     ...   \n",
      "995                 Burundi            374-180-9047  001-084-102-8353x58108   \n",
      "996                 Burundi   +1-472-150-7033x46672   +1-626-898-1897x07198   \n",
      "997                 Burundi  001-416-240-5397x41745              9075786058   \n",
      "998              Mozambique       (295)611-6638x365  001-928-405-0118x51588   \n",
      "999                 Burundi               647787401            139.476.1068   \n",
      "\n",
      "                             Email Subscription Date  \\\n",
      "0       rollinscristian@harvey.com        14-05-2020   \n",
      "1         priscilla21@richmond.com        31-03-2020   \n",
      "2         mcculloughkylie@ford.com        24-04-2022   \n",
      "3    lesterguy@figueroa-molina.com        09-10-2020   \n",
      "4               caleb30@mccann.com        02-06-2021   \n",
      "..                             ...               ...   \n",
      "995            dboyle@donovan.info        15-03-2022   \n",
      "996     jeffreyharding@johnson.com        05-02-2020   \n",
      "997           jonesjacob@downs.com        03-04-2020   \n",
      "998               cesar60@bean.com        06-11-2020   \n",
      "999      blandry@henson-harris.biz        26-04-2021   \n",
      "\n",
      "                              Website  cluster_label  \n",
      "0    http://www.nielsen-davidson.com/              0  \n",
      "1               http://www.grant.com/              0  \n",
      "2             https://www.osborn.com/              0  \n",
      "3       http://davenport-vincent.org/              0  \n",
      "4                 https://kelley.biz/              0  \n",
      "..                                ...            ...  \n",
      "995            https://www.james.com/             99  \n",
      "996        http://mcdowell-henry.com/             99  \n",
      "997     http://www.kaufman-braun.net/             99  \n",
      "998    http://www.byrd-dougherty.net/             99  \n",
      "999          http://www.humphrey.org/             99  \n",
      "\n",
      "[1000 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def cluster_text_column(df: pd.DataFrame, column_name: str, k_min: int = 2, k_max: int = 40, plot: bool = True) -> pd.DataFrame:\n",
    "    comments = df[column_name].dropna().astype(str).tolist()\n",
    "\n",
    "    # Sentence embeddings\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    X = model.encode(comments)\n",
    "\n",
    "    # Find the best number of clusters using silhouette scores\n",
    "    k_range1 = list(range(k_min, 11,2))\n",
    "    k_range2 = list(range(12,25,3))\n",
    "    k_range3 = list(range(28,40,4))\n",
    "    k_range= k_range1+k_range1+k_range2+k_range3\n",
    "    scores = silhouetteScores(k_range,X)\n",
    "    print(scores)\n",
    "    best_k_range =dict(sorted(scores.items(), key=lambda x: x[1], reverse=True)[:2])\n",
    "    print(best_k_range)\n",
    "    \n",
    "    k_scores = {}\n",
    "    for i in best_k_range.keys():\n",
    "        best_k = silhouetteScores(list(range(int(i) - 1, int(i) + 2)), X)\n",
    "        max_k = max(best_k, key=best_k.get)\n",
    "        k_scores[int(max_k)] = best_k[max_k]\n",
    "        if best_k[max_k] == 1.0:\n",
    "            break\n",
    "\n",
    "    final_k = max(k_scores, key=k_scores.get)\n",
    "    best_score = k_scores[final_k]\n",
    "\n",
    "    print(f\"\\nBest K: {final_k}, Highest Silhouette Score: {best_score:.4f}\")\n",
    "\n",
    "    final_model = KMeans(n_clusters=final_k, random_state=42, n_init=20, max_iter=550)\n",
    "    final_labels = final_model.fit_predict(X)\n",
    "\n",
    "    df_clustered = df.copy()\n",
    "    df_clustered[\"cluster_label\"] = -1\n",
    "    df_clustered.loc[df[column_name].notna(), \"cluster_label\"] = final_labels\n",
    "    df_clustered = df_clustered.sort_values(by='cluster_label',ignore_index=True)\n",
    "    \n",
    "    return df_clustered\n",
    "\n",
    "def silhouetteScores(k_range, X):\n",
    "    if 1 in k_range:\n",
    "        k_range.remove(1)\n",
    "    scores = {}\n",
    "    a = 0\n",
    "    index = 0\n",
    "    while index < len(k_range):\n",
    "        k = k_range[index]\n",
    "\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=\"auto\")\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        score = silhouette_score(X, labels)\n",
    "        scores[k] = score\n",
    "        print(f\"K={k}, Silhouette Score={score:.4f}\")\n",
    "\n",
    "        # Check conditions and expand k_range dynamically\n",
    "        if len(scores.values()) >= 14 and all(x < 0.7 for x in scores.values()):\n",
    "            print(\"nye\")\n",
    "            k_range = list(range(k + 6, 170, 6))\n",
    "            continue\n",
    "        \n",
    "        if len(scores.values()) >= 10 and all(x < 0.48 for x in scores.values()):\n",
    "            print(\"kye\")\n",
    "            k_range = list(range(k + 6, 112, 5))\n",
    "            continue\n",
    "        \n",
    "        if len(scores.values()) >= 7 and all(x < 0.32 for x in scores.values()):\n",
    "            print(\"bye\")\n",
    "            k_range = list(range(k + 5, 90, 4))\n",
    "            continue\n",
    "        \n",
    "        if len(scores.values()) >= 2:\n",
    "            if all(x < 0.25 for x in scores.values()):\n",
    "                print(\"hii\")\n",
    "                k_range = list(range(k + 4, 60, 3))\n",
    "                continue\n",
    "            values = list(scores.values())\n",
    "            if values[a] > values[a + 1] or len(set(values[-2:])) == 1:\n",
    "                return scores\n",
    "            \n",
    "            a += 1\n",
    "        index += 1\n",
    "    \n",
    "    return scores\n",
    "\n",
    "df = pd.read_csv('Files/customers-1000.csv') \n",
    "df = cluster_text_column(df,'Country')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec85ab4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can only use .dt accessor with datetimelike values\n",
      "last index called\n",
      "[368, 632]\n",
      "['Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of F' 'Group of F'\n",
      " 'Group of F' 'Group of F' 'Group of F' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R'\n",
      " 'Group of R' 'Group of R' 'Group of R' 'Group of R' 'Group of R']\n",
      "Multicalled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tikes\\AppData\\Local\\Temp\\ipykernel_20228\\1671503201.py:92: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  last_indices = df.groupby('year').apply(lambda x: x.index[-1]).to_dict()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>Employee_ID</th>\n",
       "      <th>Start_Date</th>\n",
       "      <th>Student_Names</th>\n",
       "      <th>Roll No.</th>\n",
       "      <th>Grade</th>\n",
       "      <th>f2</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Group</th>\n",
       "      <th>Sr No.</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Group of F</th>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>E7771</td>\n",
       "      <td>2024-12-09</td>\n",
       "      <td>Donald Contreras</td>\n",
       "      <td>524613</td>\n",
       "      <td>B+</td>\n",
       "      <td>-0.0930</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>E6811</td>\n",
       "      <td>2025-01-23</td>\n",
       "      <td>Richard Farrell</td>\n",
       "      <td>566434</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6412</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>E4669</td>\n",
       "      <td>2024-12-28</td>\n",
       "      <td>William Preston</td>\n",
       "      <td>578153</td>\n",
       "      <td>D</td>\n",
       "      <td>0.7002</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>E9357</td>\n",
       "      <td>2024-12-17</td>\n",
       "      <td>Anthony Smith</td>\n",
       "      <td>557050</td>\n",
       "      <td>C</td>\n",
       "      <td>0.4346</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F</td>\n",
       "      <td>E2090</td>\n",
       "      <td>2024-12-26</td>\n",
       "      <td>Daniel Jones</td>\n",
       "      <td>598470</td>\n",
       "      <td>B+</td>\n",
       "      <td>0.4112</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Group of R</th>\n",
       "      <th>995</th>\n",
       "      <td>R</td>\n",
       "      <td>E6059</td>\n",
       "      <td>2024-12-24</td>\n",
       "      <td>John Bradford</td>\n",
       "      <td>584581</td>\n",
       "      <td>A</td>\n",
       "      <td>0.8152</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>R</td>\n",
       "      <td>E8883</td>\n",
       "      <td>2025-01-16</td>\n",
       "      <td>Christopher Brown</td>\n",
       "      <td>575991</td>\n",
       "      <td>B</td>\n",
       "      <td>0.9424</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>R</td>\n",
       "      <td>E4518</td>\n",
       "      <td>2024-12-18</td>\n",
       "      <td>Adrian Taylor</td>\n",
       "      <td>537931</td>\n",
       "      <td>C</td>\n",
       "      <td>0.9920</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>R</td>\n",
       "      <td>E9541</td>\n",
       "      <td>2024-12-22</td>\n",
       "      <td>Jessica Garcia</td>\n",
       "      <td>555035</td>\n",
       "      <td>D</td>\n",
       "      <td>0.5414</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>R</td>\n",
       "      <td>E9115</td>\n",
       "      <td>2025-01-30</td>\n",
       "      <td>Mary Young</td>\n",
       "      <td>528216</td>\n",
       "      <td>B</td>\n",
       "      <td>0.3182</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  STATUS Employee_ID Start_Date      Student_Names  Roll No.  \\\n",
       "Group      Sr No.                                                              \n",
       "Group of F 0           F       E7771 2024-12-09   Donald Contreras    524613   \n",
       "           1           F       E6811 2025-01-23    Richard Farrell    566434   \n",
       "           2           F       E4669 2024-12-28    William Preston    578153   \n",
       "           3           F       E9357 2024-12-17      Anthony Smith    557050   \n",
       "           4           F       E2090 2024-12-26       Daniel Jones    598470   \n",
       "...                  ...         ...        ...                ...       ...   \n",
       "Group of R 995         R       E6059 2024-12-24      John Bradford    584581   \n",
       "           996         R       E8883 2025-01-16  Christopher Brown    575991   \n",
       "           997         R       E4518 2024-12-18      Adrian Taylor    537931   \n",
       "           998         R       E9541 2024-12-22     Jessica Garcia    555035   \n",
       "           999         R       E9115 2025-01-30         Mary Young    528216   \n",
       "\n",
       "                  Grade      f2  year  \n",
       "Group      Sr No.                      \n",
       "Group of F 0         B+ -0.0930  2017  \n",
       "           1          A  0.6412  2011  \n",
       "           2          D  0.7002  2011  \n",
       "           3          C  0.4346  2011  \n",
       "           4         B+  0.4112  2011  \n",
       "...                 ...     ...   ...  \n",
       "Group of R 995        A  0.8152  2011  \n",
       "           996        B  0.9424  2011  \n",
       "           997        C  0.9920  2011  \n",
       "           998        D  0.5414  2011  \n",
       "           999        B  0.3182  2014  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "df = pd.read_excel('Files/All.xlsx') \n",
    "asc=True\n",
    "df= digitSorting(df,col='STATUS',asc=asc)\n",
    "type={'1':10,'0':9}\n",
    "df = multiIndex(dataFrame=df,colToCheck='STATUS',colType=type,asc=asc)  \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8294a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiIndex(dataFrame, colToCheck, colType, yearOnly = False, asc=True):\n",
    "    # Step 1: Get last indices of each years\n",
    "    skip= False\n",
    "    if yearOnly == True:\n",
    "        yearsWithLastIndex = get_last_indices_of_each_year(dataFrame[colToCheck], True)\n",
    "    else: \n",
    "        if (colType['0'] == 30 or  colType['1'] == 70) or (colType['0'] == 20 or  colType['1'] == 80):\n",
    "            yearsWithLastIndex = get_last_indices_of_each_year(pd.to_datetime(dataFrame[colToCheck]),False)\n",
    "        elif(colType['1']==30 or colType['0']==70) or (colType['1'] ==50 or colType['0']==50):\n",
    "            yearsWithLastIndex = get_last_indices_of_each_year(dataFrame[colToCheck],False,True,asc=asc)\n",
    "            asc=True\n",
    "        elif(colType['1']==60 or colType['0']==40):\n",
    "            skip = True\n",
    "        else:   \n",
    "            yearsWithLastIndex = get_last_indices_of_each_year(dataFrame[colToCheck], False)\n",
    "        if asc==False and skip !=True:\n",
    "            yearsWithLastIndex = dict(reversed(list(yearsWithLastIndex.items())))\n",
    "            \n",
    "    if skip ==False:   \n",
    "        \n",
    "        nameOfGroups = list(yearsWithLastIndex.keys())\n",
    "        last_indices = list(yearsWithLastIndex.values())\n",
    "\n",
    "        # Step 2: Compute counts from last indices\n",
    "        group_sizes = []\n",
    "        prev = -1\n",
    "        for idx in last_indices:\n",
    "            group_sizes.append(idx - prev)\n",
    "            prev = idx\n",
    "        print(group_sizes)\n",
    "        # Step 3: Create array per group\n",
    "        objOfGroups = {\n",
    "            f'key{i}': np.array([f'Group of {year}'] * group_sizes[i]) for i, year in enumerate(nameOfGroups)\n",
    "        }\n",
    "    \n",
    "        # Step 4: Combine into one array\n",
    "        outside = np.concatenate(list(objOfGroups.values()))\n",
    "    else:\n",
    "        outside = np.array(df['cluster_label'])\n",
    "    \n",
    "    inside = np.arange(len(outside))\n",
    "\n",
    "    print(outside)\n",
    "    # Step 5: Create inside index\n",
    "    \n",
    "    multi_index = pd.MultiIndex.from_arrays([outside, inside], names=[\"Group\", \"Sr No.\"])\n",
    "    \n",
    "    dataFrame = dataFrame.reindex(range(len(multi_index)))\n",
    "    dataFrame.set_index(multi_index,inplace=True,)\n",
    "    print(\"Multicalled\")\n",
    "    return dataFrame\n",
    "\n",
    "def get_last_indices_of_each_year(date_series, YearOnly=False,rol=False, a=0.60,asc=True):\n",
    "    \n",
    "    # data_series = data_series.apply(pd.to_numeric, errors='coerce').astype('Int64')\n",
    "    if rol==True and findDuplicate(date_series).count() <=10:\n",
    "            df = pd.DataFrame({'year':date_series},index=np.arange(len(date_series))).astype(str)\n",
    "            df['half'] = df['year'].apply(lambda x: x[:round(len(str(x))*a)])\n",
    "            print(a)\n",
    "            last_indices =  df.groupby('half',).apply(lambda x: x.index[-1]).to_dict()\n",
    "            if asc ==False:\n",
    "                last_indices = dict(reversed(list(last_indices.items())))\n",
    "                \n",
    "            group_sizes = []\n",
    "            prev = -1\n",
    "            k=0\n",
    "            for i,idx in enumerate(last_indices.values()):\n",
    "                group_sizes.append(idx - prev)\n",
    "                prev = idx\n",
    "                if group_sizes[i]<= 15:\n",
    "                    k=k+1\n",
    "            print(k)\n",
    "            if round(len(last_indices)*0.45)<=k and a>=0.20:\n",
    "                last_indices = get_last_indices_of_each_year(date_series,False,True,a=round(a-0.10,3), asc=asc)\n",
    "            return last_indices\n",
    "    if YearOnly == True:\n",
    "        print(\"yearOnly work\")\n",
    "        df = pd.DataFrame({'year': date_series}, index=np.arange(len(date_series)))\n",
    "    \n",
    "    else:\n",
    "        # Extract year\n",
    "        try:\n",
    "            years = date_series.dt.year\n",
    "            # Create a DataFrame with index\n",
    "            df = pd.DataFrame({'year': years}, index=np.arange(len(date_series)))\n",
    "            print(\"Year Extracted\")\n",
    "        except Exception as e:\n",
    "            #Create a DataFrame with index\n",
    "            print(e)\n",
    "            df = pd.DataFrame({'year': date_series}, index=np.arange(len(date_series)))\n",
    "        \n",
    "    # Get last index of each year group\n",
    "    last_indices = df.groupby('year').apply(lambda x: x.index[-1]).to_dict()\n",
    "    print(\"last index called\")\n",
    "    return last_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9248e84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([('e,', 1),\n",
       "            ( 'e', 2),\n",
       "            ( 't', 3),\n",
       "            ( 't', 4)],\n",
       "           )"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['e,','e','t','t']\n",
    "b = np.array([1,2,3,4])\n",
    "m = pd.MultiIndex.from_arrays([a,b])\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1392d60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sl.no.  U-DISE Code Name of Monastic School under SSA\n",
      "0       39  11010100106                  NAGA MONASTIC PS\n",
      "1       40  11010100302            SINGCHIT NADAK MON. PS\n",
      "2       41  11010100602                       SINGHIK MPS\n",
      "3       38  11010104301                    RINGHIM MON PS\n",
      "4       31  11010200105     LACHEN NGEDUP CHOLING MON. PS\n",
      "..     ...          ...                               ...\n",
      "74      24  11040901404           CHAGSAM MONASTIC SCHOOL\n",
      "75      15  11041002003              SANG MONASTIC SCHOOL\n",
      "76      16  11041002004    SANG NORBULING MONASTIC SCHOOL\n",
      "77      14  11041002102          CHUNKHAR MONASTIC SCHOOL\n",
      "78      19  11041100105    LINKEY MACHONG MONASTIC SCHOOL\n",
      "\n",
      "[79 rows x 3 columns]\n",
      "0.6\n",
      "24\n",
      "0.5\n",
      "24\n",
      "0.4\n",
      "1\n",
      "[18, 13, 20, 28]\n",
      "['Group of 1101' 'Group of 1101' 'Group of 1101' 'Group of 1101'\n",
      " 'Group of 1101' 'Group of 1101' 'Group of 1101' 'Group of 1101'\n",
      " 'Group of 1101' 'Group of 1101' 'Group of 1101' 'Group of 1101'\n",
      " 'Group of 1101' 'Group of 1101' 'Group of 1101' 'Group of 1101'\n",
      " 'Group of 1101' 'Group of 1101' 'Group of 1102' 'Group of 1102'\n",
      " 'Group of 1102' 'Group of 1102' 'Group of 1102' 'Group of 1102'\n",
      " 'Group of 1102' 'Group of 1102' 'Group of 1102' 'Group of 1102'\n",
      " 'Group of 1102' 'Group of 1102' 'Group of 1102' 'Group of 1103'\n",
      " 'Group of 1103' 'Group of 1103' 'Group of 1103' 'Group of 1103'\n",
      " 'Group of 1103' 'Group of 1103' 'Group of 1103' 'Group of 1103'\n",
      " 'Group of 1103' 'Group of 1103' 'Group of 1103' 'Group of 1103'\n",
      " 'Group of 1103' 'Group of 1103' 'Group of 1103' 'Group of 1103'\n",
      " 'Group of 1103' 'Group of 1103' 'Group of 1103' 'Group of 1104'\n",
      " 'Group of 1104' 'Group of 1104' 'Group of 1104' 'Group of 1104'\n",
      " 'Group of 1104' 'Group of 1104' 'Group of 1104' 'Group of 1104'\n",
      " 'Group of 1104' 'Group of 1104' 'Group of 1104' 'Group of 1104'\n",
      " 'Group of 1104' 'Group of 1104' 'Group of 1104' 'Group of 1104'\n",
      " 'Group of 1104' 'Group of 1104' 'Group of 1104' 'Group of 1104'\n",
      " 'Group of 1104' 'Group of 1104' 'Group of 1104' 'Group of 1104'\n",
      " 'Group of 1104' 'Group of 1104' 'Group of 1104']\n",
      "Multicalled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tikes\\AppData\\Local\\Temp\\ipykernel_7736\\416729770.py:52: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  last_indices =  df.groupby('half',).apply(lambda x: x.index[-1]).to_dict()\n",
      "C:\\Users\\tikes\\AppData\\Local\\Temp\\ipykernel_7736\\416729770.py:52: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  last_indices =  df.groupby('half',).apply(lambda x: x.index[-1]).to_dict()\n",
      "C:\\Users\\tikes\\AppData\\Local\\Temp\\ipykernel_7736\\416729770.py:52: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  last_indices =  df.groupby('half',).apply(lambda x: x.index[-1]).to_dict()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Sl.no.</th>\n",
       "      <th>U-DISE Code</th>\n",
       "      <th>Name of Monastic School under SSA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Group</th>\n",
       "      <th>Sr No.</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Group of 1101</th>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>11010100106</td>\n",
       "      <td>NAGA MONASTIC PS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>11010100302</td>\n",
       "      <td>SINGCHIT NADAK MON. PS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>11010100602</td>\n",
       "      <td>SINGHIK MPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>11010104301</td>\n",
       "      <td>RINGHIM MON PS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>11010200105</td>\n",
       "      <td>LACHEN NGEDUP CHOLING MON. PS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Group of 1104</th>\n",
       "      <th>74</th>\n",
       "      <td>24</td>\n",
       "      <td>11040901404</td>\n",
       "      <td>CHAGSAM MONASTIC SCHOOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>15</td>\n",
       "      <td>11041002003</td>\n",
       "      <td>SANG MONASTIC SCHOOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>16</td>\n",
       "      <td>11041002004</td>\n",
       "      <td>SANG NORBULING MONASTIC SCHOOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>14</td>\n",
       "      <td>11041002102</td>\n",
       "      <td>CHUNKHAR MONASTIC SCHOOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>19</td>\n",
       "      <td>11041100105</td>\n",
       "      <td>LINKEY MACHONG MONASTIC SCHOOL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Sl.no.  U-DISE Code Name of Monastic School under SSA\n",
       "Group         Sr No.                                                       \n",
       "Group of 1101 0           39  11010100106                  NAGA MONASTIC PS\n",
       "              1           40  11010100302            SINGCHIT NADAK MON. PS\n",
       "              2           41  11010100602                       SINGHIK MPS\n",
       "              3           38  11010104301                    RINGHIM MON PS\n",
       "              4           31  11010200105     LACHEN NGEDUP CHOLING MON. PS\n",
       "...                      ...          ...                               ...\n",
       "Group of 1104 74          24  11040901404           CHAGSAM MONASTIC SCHOOL\n",
       "              75          15  11041002003              SANG MONASTIC SCHOOL\n",
       "              76          16  11041002004    SANG NORBULING MONASTIC SCHOOL\n",
       "              77          14  11041002102          CHUNKHAR MONASTIC SCHOOL\n",
       "              78          19  11041100105    LINKEY MACHONG MONASTIC SCHOOL\n",
       "\n",
       "[79 rows x 3 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "import pandas as pd\n",
    "df = pd.read_csv('Files/201516ListofMonasticSchools_1_0.csv')  # Your dataset with 'Roll No.'\n",
    "asc = True\n",
    "df = sortRollCol(df,'U-DISE Code',asc=asc)\n",
    "\n",
    "print(df)\n",
    "type = {\n",
    "        '0' : 70, \n",
    "        '1' : 30\n",
    "        }\n",
    "df = multiIndex(df,colToCheck='U-DISE Code',colType=type,asc=asc)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91fbbf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit\n",
    "import pandas as pd\n",
    "import math\n",
    "def detectColumns(df, prioColumns):\n",
    "    result = {}    \n",
    "    for col in prioColumns:\n",
    "        qc =  QuantumCircuit(1,1)\n",
    "        col_data = df[col]\n",
    "        col_str = df[col].astype(str).str.strip()\n",
    "        # 3. Check for Date values (type 2)\n",
    "        if check_date_format(col_str) :\n",
    "            p = 0.70000\n",
    "                \n",
    "        # 4. Check for DateTime values (type 3)\n",
    "        elif check_datetime_format(col_str):\n",
    "            p = 0.800000\n",
    "            \n",
    "        elif OneOr2digitDetection(col_data) and detectIdTypeCol(col_data)==False:\n",
    "            p = 0.100000\n",
    "        # 1. Check for Roll Numbers (type 4)\n",
    "        elif pd.api.types.is_numeric_dtype(col_data):\n",
    "            p = 0.2000\n",
    "            # 2. Check for Year values (type 1)\n",
    "            if check_year_values(col_data):\n",
    "                p = 0.40000\n",
    "            elif check_roll_number(col_data):\n",
    "                p = 0.30000\n",
    "\n",
    "        elif pd.api.types.is_string_dtype(df[col]) or pd.api.types.is_object_dtype(df[col]):\n",
    "            p = 0.600000\n",
    "            if detectIdTypeCol(col_data):\n",
    "                p = 0.5\n",
    "        \n",
    "        angle = 2 * math.asin(math.sqrt(p))\n",
    "        qc.ry(angle, 0)\n",
    "        # Initialize result as 0 (unrecognized type)\n",
    "        result[col] = measurCir(qc,0)\n",
    "        \n",
    "    return result\n",
    "\n",
    "def check_roll_number(col_data):\n",
    "    try:\n",
    "        # Convert numbers to strings for checking patterns\n",
    "        sample_start = [str(i) for i in col_data.head(10)]\n",
    "        sample_middle = [str(i) for i in col_data.iloc[int(len(col_data)/2)-5:int(len(col_data)/2)+5]]\n",
    "        sample_end = [str(i) for i in col_data.iloc[-10:]]\n",
    "        \n",
    "        # Combine samples\n",
    "        samples = sample_start + sample_middle + sample_end\n",
    "        \n",
    "        # Check if all numbers have the same length and >= 5 digits\n",
    "        if len(set(len(str(x)) for x in samples)) == 1:\n",
    "            length = len(str(samples[0]))\n",
    "            if length >= 5:\n",
    "                # Check if all numbers start with the same digit\n",
    "                first_digits = str(samples[0])[0]\n",
    "                return all(str(x).startswith(first_digit) for x in samples)\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def check_year_values(col_data):\n",
    "    if pd.api.types.is_string_dtype(col_data):\n",
    "        try:\n",
    "            col_data = pd.to_numeric(col_data)\n",
    "        except:\n",
    "            pass\n",
    "    # Handle if column is numeric and looks like a year\n",
    "    if pd.api.types.is_numeric_dtype(col_data) or  pd.api.types.is_float_dtype(col_data) or  pd.api.types.is_integer_dtype(col_data):\n",
    "        if col_data.dropna().empty == False:\n",
    "            if pd.api.types.is_float_dtype(col_data):\n",
    "            # Check if float values have only 2 decimal places\n",
    "                if col_data.dropna().apply(lambda x: round(x, 2) == x).all():\n",
    "                    if col_data.dropna().between(1800, 2050).all():\n",
    "                        return True # Only year\n",
    "            # For integer values\n",
    "            elif col_data.dropna().between(1800, 2100).all():\n",
    "                True # Only year\n",
    "                \n",
    "    return False\n",
    "\n",
    "def check_date_format(col_str):\n",
    "    # Check for common date formats (yyyy-mm-dd, dd-mm-yyyy, etc.)\n",
    "    date_pattern = r'^\\d{1,4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,4}$'\n",
    "    return col_str.str.match(date_pattern).all()\n",
    "\n",
    "def check_datetime_format(col_str):\n",
    "    # Check for datetime format (date + time)\n",
    "    datetime_pattern = r'\\d{1,4}[-/\\.]\\d{1,2}[-/\\.]\\d{1,4}.*\\d{1,2}:\\d{2}'\n",
    "    return col_str.str.contains(datetime_pattern).all()\n",
    "\n",
    "def detectIdTypeCol(col_data):\n",
    "    if pd.api.types.is_string_dtype(col_data):\n",
    "        pattern = r'\\b[A-Z0-9]{1,4}[-_./]?[A-Z0-9]{2,6}[-_./]?[A-Z0-9]{0,5}\\b'\n",
    "        return all(re.fullmatch(pattern, item) for item in col_data)\n",
    "    \n",
    "def OneOr2digitDetection(col_data):\n",
    "    non2Digit=0\n",
    "    for i,v  in enumerate(col_data):\n",
    "        if len(str(v)) > 2 and str(v)!='nan':\n",
    "            non2Digit=non2Digit+1\n",
    "    if len(col_data)/2.2 > non2Digit:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def detectSimpleDtypes(col_data):\n",
    "    if pd.api.types.is_integer_dtype(col_data):\n",
    "        return 'allInt'\n",
    "    if pd.api.types.is_float_dtype(col_data):\n",
    "        if col_data.isna().all()== False:\n",
    "            return 'numaric'\n",
    "    if pd.api.types.is_string_dtype(col_data) or pd.api.types.is_object_dtype(col_data):\n",
    "        return 'str'\n",
    "    return None\n",
    "\n",
    "def clusterDateTimeCol(fContent, col,no,ascending=True):\n",
    "    if no ==1:\n",
    "        # Try to detect and sort if the column is just year values\n",
    "        fContent = fContent.sort_values(by=col,ignore_index=True, ascending=ascending)\n",
    "        fContent = fContent.reset_index(drop=True)\n",
    "    elif no == 2 or 3:\n",
    "        # Now, try to detect proper date/datetime columns\n",
    "        try:\n",
    "            # Avoid processing numeric-only or zero-filled columns\n",
    "            sample_vals = fContent[col].astype(str).str.strip().replace('0', np.nan).dropna()\n",
    "            if len(sample_vals) == 0:\n",
    "                return fContent# All values are zero or empty-like\n",
    "            \n",
    "            # Try parsing\n",
    "            try:\n",
    "                parsed_col = pd.to_datetime(fContent[col], errors='raise')\n",
    "                print('try')\n",
    "            except Exception as e:\n",
    "                parsed_col = pd.to_datetime(fContent[col], errors='raise',dayfirst=True)\n",
    "                print(e)\n",
    "               \n",
    "            if all(parsed_col.dt.time == pd.to_datetime('00:00:00').time()) and no == 2:  # Only date\n",
    "                fContent[col] = parsed_col\n",
    "                fContent = clean_and_sort_date_column(fContent, col, ascending)\n",
    "                fContent = fContent.reset_index(drop=True)\n",
    "                # Replace original column with parsed datetime values\n",
    "            elif no == 3:  # Date + time\n",
    "                fContent[col] = parsed_col\n",
    "                fContent = handle_datetime_column(fContent, col, ascending)\n",
    "                fContent = fContent.reset_index(drop=True)\n",
    "        except Exception as e:\n",
    "            return fContent # Not a datetime column\n",
    "        \n",
    "    return fContent \n",
    "\n",
    "def clean_and_sort_date_column(dff, column_name, ascending=True):\n",
    "        try:\n",
    "            \n",
    "            # Step 2: Drop NaT (invalid formats)\n",
    "            dff = dff.dropna(subset=[column_name])\n",
    "            \n",
    "            # Step 3: Sort the DataFrame by that column\n",
    "            dff = dff.sort_values(by=column_name, ascending=ascending)\n",
    "\n",
    "            # Optional: Format to clean date string (YYYY-MM-DD)\n",
    "            dff[column_name] = dff[column_name].dt.strftime('%Y-%m-%d')\n",
    "            print(f\"{column_name} date called \")\n",
    "            return dff\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error while processing date column: {e}\")\n",
    "            return dff\n",
    "\n",
    "def handle_datetime_column(df, column_name, ascending):\n",
    "    print(f\"{column_name} dateTime\")\n",
    "    # Check if most values in column are datetime with time\n",
    "    values = df[column_name].dropna().astype(str).head(20)\n",
    "    count_datetime = sum([pd.api.is_datetime(v) for v in values])\n",
    "\n",
    "    if count_datetime >= len(values) // 2:  # At least half must be datetime-like\n",
    "        # Convert full column to datetime\n",
    "        df[column_name] = pd.to_datetime(df[column_name], errors='coerce')\n",
    "        # Drop rows with invalid dates\n",
    "        df = df.dropna(subset=[column_name])\n",
    "        # Sort by that column\n",
    "        df = df.sort_values(by=column_name,ascending=ascending).reset_index(drop=True)\n",
    "        print(f\"[INFO] '{column_name}' successfully recognized and sorted as datetime.\")\n",
    "    else:\n",
    "        print(f\"[INFO] '{column_name}' does not contain proper datetime with time.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26207e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measurCir(qc,q_num):\n",
    "    qc.measure(q_num,q_num)\n",
    "    simulator = AerSimulator()\n",
    "    # Transpile & run\n",
    "    compiled = transpile(qc, simulator)\n",
    "    r = simulator.run(compiled, shots=100000).result()\n",
    "    counts = r.get_counts()\n",
    "    for k,v in counts.items():\n",
    "        counts[k] = int(v/1000)\n",
    "    return counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
